{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "\n",
    "1. **Using VEM/FEM Solutions for Efficient Training**:\n",
    "   - By training the neural network on the displacement field computed using VEM/FEM, you're providing the model with a high-quality reference solution. This allows the model to learn the underlying physical relationships between the parameters (such as Young’s modulus \\(E\\), cross-sectional area \\(A\\), and moment of inertia \\(I\\)) and the displacement field.\n",
    "\n",
    "2. **Generalization with Fewer Data**:\n",
    "   - Since the model is grounded in physically informed solutions, you likely need **fewer training examples** to generalize to new material and geometrical configurations. Unlike traditional machine learning models that require vast amounts of labeled data, your approach can rely on solving a **few instances** of VEM/FEM solutions and using that information to generalize.\n",
    "\n",
    "3. **Parameter Sensitivity and Inference**:\n",
    "   - The network’s sensitivity to material and geometrical parameters (\\(E\\), \\(A\\), \\(I\\)) is key. Once trained, the model will allow for **rapid inference** with new combinations of these parameters without needing to solve the full VEM/FEM system again.\n",
    "   - In an engineering context, this is particularly advantageous, as engineers often need to explore various material or geometric configurations during design optimization. Having a trained neural network that provides **instant predictions** without solving a full VEM/FEM problem would significantly improve efficiency.\n",
    "\n",
    "4. **Efficiency Compared to Traditional VEM/FEM**:\n",
    "   - Solving a full VEM/FEM problem repeatedly for different parameter values can be computationally expensive, especially for large or complex systems. By training a neural network to approximate the displacement field based on these parameters, you essentially create a **surrogate model** that can make predictions more efficiently.\n",
    "\n",
    "### Challenges and Considerations:\n",
    "- **Accuracy vs. Efficiency**: While the neural network may provide fast predictions, the trade-off is the potential for reduced accuracy compared to solving the full VEM/FEM system. This can be mitigated by fine-tuning the network and introducing additional regularization techniques like Sobolev training.\n",
    "  \n",
    "- **Extrapolation Limits**: The network might struggle with extrapolating far beyond the range of material and geometrical parameters it was trained on. Ensuring that the training data includes a representative range of parameters will be crucial for reliable generalization.\n",
    "\n",
    "- **Hybrid Model Validation**: You could validate your hypothesis by comparing the **computational cost** (in terms of time) and **accuracy** between solving multiple VEM/FEM instances and using the trained neural network for inference over a variety of material/geometrical configurations.\n",
    "\n",
    "### Conclusion:\n",
    "The approach of training a neural network using VEM/FEM solutions to enable efficient inference of displacement fields for different material and geometric configurations is a practical and promising solution in engineering contexts. It leverages the strengths of both numerical methods and machine learning to balance accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import core.vem as vem\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "import utils.mesh as mesh\n",
    "import core.loss as loss_function\n",
    "import core.errors as errors\n",
    "import core.neural_backend as neural\n",
    "\n",
    "import solve_vem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available!\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS backend is available!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS backend is not available. Using CPU.\")\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of elements per edge\n",
    "num_elements_per_edge = 32\n",
    "\n",
    "# geometry data\n",
    "L = 2.0\n",
    "I = 1e-4\n",
    "A = 1\n",
    "\n",
    "# material data\n",
    "E = 27e6\n",
    "\n",
    "# Define load parameters\n",
    "q = -400\n",
    "t = 0\n",
    "\n",
    "# Time sampling size\n",
    "time_sampling_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving time [s]: 0.0019\n",
      "######################### Beam ##########################\n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -6.11294039e-19\n",
      " -1.94881114e-06  2.00665681e-18 -1.07225777e-18 -3.89762227e-06\n",
      " -4.81056994e-18 -1.20533083e-18 -5.84643341e-06 -8.48584458e-18\n",
      " -1.20465840e-18 -7.79524455e-06 -9.09042567e-18 -1.17311919e-18\n",
      " -9.74405568e-06 -9.47358127e-18 -1.20009988e-18 -1.16928668e-05\n",
      " -7.21778913e-18 -1.06211412e-18 -1.36416780e-05 -1.47525077e-17\n",
      " -1.56704915e-18 -1.55904891e-05  1.33556765e-17  2.95131067e-19\n",
      " -1.75393002e-05 -9.05001960e-17 -6.58894322e-18 -1.94881114e-05\n",
      "  2.93235821e-16  1.88480147e-17 -2.14369225e-05 -1.12477323e-15\n",
      " -7.51477879e-17 -2.33857336e-05  4.11508416e-15  2.72187318e-16\n",
      " -2.53345448e-05 -1.52473623e-14 -1.01129425e-15 -2.72833559e-05\n",
      "  5.63012177e-14  3.73145723e-15 -2.92321670e-05 -2.08086820e-13\n",
      " -1.37940730e-14 -3.11809782e-05  7.68886231e-13  5.09666882e-14\n",
      " -3.31297893e-05 -2.84124814e-12 -1.88338838e-13 -3.50786005e-05\n",
      "  1.04990074e-11  6.95948712e-13 -3.70274116e-05 -3.87962266e-11\n",
      " -2.57169197e-12 -3.89762227e-05  1.43360711e-10  9.50297265e-12\n",
      " -4.09250339e-05 -5.29749996e-10 -3.51156192e-11 -4.28738450e-05\n",
      "  1.95754492e-09  1.29760077e-10 -4.48226561e-05 -7.23356722e-09\n",
      " -4.79492571e-10 -4.67714673e-05  2.67296519e-08  1.77183250e-09\n",
      " -4.87202784e-05 -9.87720544e-08 -6.54731819e-09 -5.06690896e-05\n",
      "  3.64984877e-07  2.41938080e-08 -5.26179007e-05 -1.34870092e-06\n",
      " -8.94015428e-08 -5.45667118e-05  4.98375214e-06  3.30358736e-07\n",
      " -5.65155230e-05 -1.84160811e-05 -1.22074957e-06 -5.84643341e-05\n",
      "  6.80515468e-05  4.51094324e-06 -6.04131452e-05 -2.51465717e-04\n",
      " -1.66689462e-05 -6.23619564e-05  9.29222179e-04 -1.66689462e-05\n",
      " -2.49403084e-05  4.68257634e-04 -1.66689462e-05 -1.14713779e-05\n",
      "  1.56707057e-04 -1.66689462e-05 -1.02158223e-05  7.14014743e-05\n",
      " -1.66689462e-05 -1.13480773e-05  7.42689527e-05 -1.66689462e-05\n",
      " -1.18228193e-05  8.64137252e-05 -1.66689462e-05 -1.16199620e-05\n",
      "  8.99239695e-05 -1.66689462e-05 -1.11843858e-05  8.78325676e-05\n",
      " -1.66689462e-05 -1.07363370e-05  8.42949259e-05 -1.66689462e-05\n",
      " -1.03169442e-05  8.08874125e-05 -1.66689462e-05 -9.91340720e-06\n",
      "  7.77593785e-05 -1.66689462e-05 -9.51333441e-06  7.47483930e-05\n",
      " -1.66689462e-05 -9.11258594e-06  7.17528371e-05 -1.66689462e-05\n",
      " -8.71099027e-06  6.87473397e-05 -1.66689462e-05 -8.30908177e-06\n",
      "  6.57346839e-05 -1.66689462e-05 -7.90714701e-06  6.27200166e-05\n",
      " -1.66689462e-05 -7.50524612e-06  5.97054363e-05 -1.66689462e-05\n",
      " -7.10336572e-06  5.66911975e-05 -1.66689462e-05 -6.70149018e-06\n",
      "  5.36771172e-05 -1.66689462e-05 -6.29961428e-06  5.06630455e-05\n",
      " -1.66689462e-05 -5.89773628e-06  4.76490206e-05 -1.66689462e-05\n",
      " -5.49586171e-06  4.46347723e-05 -1.66689462e-05 -5.09397289e-06\n",
      "  4.16213132e-05 -1.66689462e-05 -4.69213661e-06  3.86049272e-05\n",
      " -1.66689462e-05 -4.29010633e-06  3.55993577e-05 -1.66689462e-05\n",
      " -3.88879303e-06  3.25538202e-05 -1.66689462e-05 -3.48483035e-06\n",
      "  2.96559744e-05 -1.66689462e-05 -3.09065773e-06  2.62123746e-05\n",
      " -1.66689462e-05 -2.66030863e-06  2.47854582e-05 -1.66689462e-05\n",
      " -2.36363974e-06  1.59064445e-05 -1.66689462e-05 -1.57299253e-06\n",
      "  3.45646005e-05 -1.66689462e-05 -2.60770588e-06 -4.85332570e-05\n",
      " -1.66689462e-05  3.10269711e-06  2.44380160e-04 -4.90681721e-06\n",
      "  3.00573783e-06  1.32007968e-04  9.26621974e-08  2.90877854e-06\n",
      "  2.79753734e-05  7.77873404e-07  2.81181926e-06 -6.04861474e-06\n",
      "  3.66021873e-07  2.71485997e-06 -7.13063427e-06  6.25963471e-08\n",
      "  2.61790069e-06 -2.57898256e-06 -2.40822363e-08  2.52094140e-06\n",
      " -1.94732114e-07 -2.11061147e-08  2.42398212e-06  2.89968006e-07\n",
      " -6.71104246e-09  2.32702283e-06  1.70674305e-07 -1.25888822e-10\n",
      "  2.23006355e-06  4.00506109e-08  9.40533016e-10  2.13310426e-06\n",
      " -5.92511206e-09  4.78084318e-10  2.03614498e-06 -8.87324628e-09\n",
      "  9.30332502e-11  1.93918569e-06 -3.44838788e-09 -2.59508058e-11\n",
      "  1.84222641e-06 -3.59101896e-10 -2.65630429e-11  1.74526713e-06\n",
      "  3.39510326e-10 -9.09682050e-12  1.64830784e-06  2.19408809e-10\n",
      " -4.76171238e-13  1.55134856e-06  5.64519850e-11  1.12569390e-12\n",
      "  1.45438927e-06 -5.19228312e-12  6.20802451e-13  1.35742999e-06\n",
      " -1.09642257e-11  1.35079797e-13  1.26047070e-06 -4.57888174e-12\n",
      " -2.67279550e-14  1.16351142e-06 -5.98948824e-13 -3.32211745e-14\n",
      "  1.06655213e-06  3.91183282e-13 -1.22329051e-14  9.69592847e-07\n",
      "  2.80458819e-13 -1.01426933e-15  8.72633563e-07  7.85550065e-14\n",
      "  1.33039671e-15  7.75674278e-07 -3.50821239e-15  8.00336905e-16\n",
      "  6.78714993e-07 -1.34362207e-14  1.91418290e-16  5.81755708e-07\n",
      " -6.03169426e-15 -2.67919063e-17  4.84796424e-07 -9.33551324e-16\n",
      " -4.24328640e-17  3.87837139e-07  4.50521385e-16 -1.74943124e-17\n",
      "  2.90877854e-07  3.64992977e-16 -2.96629836e-18  1.93918569e-07\n",
      "  1.17384181e-16  3.50978645e-19  9.69592847e-08  6.24939266e-18\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "#########################################################\n"
     ]
    }
   ],
   "source": [
    "# Generate the geometry\n",
    "nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Plot the nodes\n",
    "mesh.plot_nodes(nodes, elements)\n",
    "\n",
    "# Solve the problem using the VEM\n",
    "uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E, A, I, load, q, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Solving and Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring time for solving using VEM\n",
    "solving_time_list = []\n",
    "for _ in range(time_sampling_size):\n",
    "    uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E, A, I, load, q, t)\n",
    "    solving_time_list.append(solving_time)\n",
    "\n",
    "print(\"Mean solving time: \", np.mean(solving_time_list))\n",
    "print(\"Std Deviation: \", np.std(solving_time_list))\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 800\n",
    "\n",
    "# Layers definition\n",
    "nodes_layers = [128, 256, 512, 512, 512, 512]  # Layers for nodes sub-network\n",
    "material_layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 2048, 2048] # Layers for materials sub-network\n",
    "final_layers = [1024, 1024, 1024, 1024]  # Layers for final combination network\n",
    "\n",
    "# Training pipeline\n",
    "(input_vector, \n",
    " model, \n",
    " total_loss_values, \n",
    " loss_values, \n",
    " material_loss_values, \n",
    " sobolev_loss_values, \n",
    " alpha_values_values) = neural.train_material_portic(\n",
    "    epochs=num_epochs,\n",
    "    nodes=nodes,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    E=E,\n",
    "    A=A,\n",
    "    I=I,\n",
    "    uh_vem=uh_vem,\n",
    "    nodes_layers=nodes_layers,\n",
    "    material_layers=material_layers,\n",
    "    final_layers=final_layers,\n",
    "    verbose=True,\n",
    "    noramlize_inputs=True,\n",
    "    network_type=\"material\"\n",
    " )\n",
    "\n",
    "# Setting up material parameters\n",
    "material_params = torch.tensor([E , A , I ], dtype=torch.float32)\n",
    "# nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "\n",
    "# Measuring time spent for inference\n",
    "inference_time_list = []\n",
    "for _ in range(time_sampling_size):\n",
    "    predicted_displacements, l2_error, energy_error, h1_error, inference_time = neural.test_portic(\n",
    "        nodes=nodes,\n",
    "        material_params=material_params,\n",
    "        model=model,\n",
    "        uh_vem=uh_vem,\n",
    "        K=K,\n",
    "        f=f,\n",
    "        concatanate=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    inference_time_list.append(inference_time)\n",
    "\n",
    "print(\"Mean inference time: \", np.mean(inference_time_list))\n",
    "print(\"Std Deviation: \", np.std(inference_time_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model refers to the disaplcement field and the loss function regards to the calculation of the residual taking in consideration the Virtual Element Method's stiffness matrix and load vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of Deep Layers in the Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 800\n",
    "\n",
    "# Reading the json with respective layers\n",
    "with open(\"data/layers_20240929.json\", \"r\") as data:\n",
    "    layers = json.load(data)\n",
    "\n",
    "results = []\n",
    "\n",
    "for i,layer in enumerate(layers):\n",
    "\n",
    "    # Define the number of elements per edge\n",
    "    num_elements_per_edge = 128\n",
    "\n",
    "    # geometry data\n",
    "    L = 2.0\n",
    "    I = 1e-4\n",
    "    A = 1\n",
    "\n",
    "    # material data\n",
    "    E = 27e6\n",
    "\n",
    "    # Define load parameters\n",
    "    q = -400\n",
    "    t = 0\n",
    "\n",
    "\n",
    "    # Generate the geometry\n",
    "    nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "    # Solve the problem using the VEM\n",
    "    uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E, A, I, load, q, t, verbose=False)\n",
    "\n",
    "    print(f\"Training and testing layer {i+1}/{len(layers)}\")\n",
    "    # Defining layers\n",
    "    nodes_layers = list(layer[\"node_layers\"])\n",
    "    material_layers = list(layer[\"material_layers\"])\n",
    "    final_layers = list(layer[\"final_layers\"])\n",
    "\n",
    "    # Training pipeline\n",
    "    (input_vector, \n",
    "    model, \n",
    "    total_loss_values, \n",
    "    loss_values, \n",
    "    material_loss_values, \n",
    "    sobolev_loss_values, \n",
    "    alpha_values_values) = neural.train_material_portic(\n",
    "        epochs=num_epochs,\n",
    "        nodes=nodes,\n",
    "        K=K,\n",
    "        f=f,\n",
    "        E=E,\n",
    "        A=A,\n",
    "        I=I,\n",
    "        uh_vem=uh_vem,\n",
    "        nodes_layers=nodes_layers,\n",
    "        material_layers=material_layers,\n",
    "        final_layers=final_layers,\n",
    "        verbose=False,\n",
    "        noramlize_inputs=True,\n",
    "        network_type=\"material\"\n",
    "    )\n",
    "\n",
    "    # Setting up material parameters\n",
    "    material_params = torch.tensor([E , A , I ], dtype=torch.float32)\n",
    "    nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "\n",
    "    # Testing the model with default parameters\n",
    "    predicted_displacements, l2_error_default, energy_error_default, h1_error_default, inference_time_default = neural.test_portic(\n",
    "        nodes=nodes,\n",
    "        material_params=material_params,\n",
    "        model=model,\n",
    "        uh_vem=uh_vem,\n",
    "        K=K,\n",
    "        f=f,\n",
    "        concatanate=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Setting up new material parameters\n",
    "    I_new = 1e-4\n",
    "    A_new = 2\n",
    "    E_new = 110e5\n",
    "\n",
    "    # Generate the geometry\n",
    "    nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "    # Solve the problem using the VEM\n",
    "    uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E_new, A_new, I_new, load, q, t)\n",
    "\n",
    "    # Testing the model with new parameters\n",
    "    material_params = torch.tensor([E_new , A_new , I_new], dtype=torch.float32)\n",
    "    nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "    nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "    predicted_displacements, l2_error_new, energy_error_new, h1_error_new, inference_time_new = neural.test_portic(\n",
    "        nodes=nodes,\n",
    "        material_params=material_params,\n",
    "        model=model,\n",
    "        uh_vem=uh_vem,\n",
    "        K=K,\n",
    "        f=f,\n",
    "        concatanate=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # Setting up new material parameters\n",
    "    I_new_2 = 1e-4\n",
    "    A_new_2 = 3\n",
    "    E_new_2 = 80e3\n",
    "\n",
    "    # Generate the geometry\n",
    "    nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "    # Solve the problem using the VEM\n",
    "    uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E_new_2, A_new_2, I_new_2, load, q, t)\n",
    "\n",
    "    # Testing the model with new parameters\n",
    "    material_params = torch.tensor([E_new_2 , A_new_2 , I_new_2], dtype=torch.float32)\n",
    "    nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "    nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "    predicted_displacements, l2_error_new, energy_error_new, h1_error_new, inference_time_new = neural.test_portic(\n",
    "        nodes=nodes,\n",
    "        material_params=material_params,\n",
    "        model=model,\n",
    "        uh_vem=uh_vem,\n",
    "        K=K,\n",
    "        f=f,\n",
    "        concatanate=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"tag\": layer[\"tag\"],\n",
    "        \"l2_error_default\": l2_error_default,\n",
    "        \"energy_error_default\": energy_error_default,\n",
    "        \"h1_error_default\": h1_error_default,\n",
    "        \"inferece_time_default\": inference_time_default,\n",
    "        \"l2_error_new\": l2_error_new,\n",
    "        \"energy_error_new\": energy_error_new,\n",
    "        \"h1_error_new\": h1_error_new,\n",
    "        \"inferece_time_new\": inference_time_new,\n",
    "        \"l2_error_new_2\": l2_error_new,\n",
    "        \"energy_error_new_2\": energy_error_new,\n",
    "        \"h1_error_new_2\": h1_error_new,\n",
    "        \"inferece_time_new_2\": inference_time_new,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in a dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"data/output/results_20240929.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 800\n",
    "\n",
    "# Layers definition\n",
    "nodes_layers = [128, 256, 512, 512, 512, 512]  # Layers for nodes sub-network\n",
    "material_layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 2048, 2048] # Layers for materials sub-network\n",
    "final_layers = [1024, 1024, 1024, 1024]  # Layers for final combination network\n",
    "\n",
    "# Training pipeline\n",
    "(input_vector, \n",
    " model, \n",
    " total_loss_values, \n",
    " loss_values, \n",
    " material_loss_values, \n",
    " sobolev_loss_values, \n",
    " alpha_values_values) = neural.train_material_portic(\n",
    "    epochs=num_epochs,\n",
    "    nodes=nodes,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    E=E,\n",
    "    A=A,\n",
    "    I=I,\n",
    "    uh_vem=uh_vem,\n",
    "    nodes_layers=nodes_layers,\n",
    "    material_layers=material_layers,\n",
    "    final_layers=final_layers,\n",
    "    verbose=True,\n",
    "    noramlize_inputs=True,\n",
    "    network_type=\"material\",\n",
    "    batch_norm=False\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the reference displacement field calculated by the Virtual Element Method, a displacemente field is supposed to be calculated considering the material parameters contributions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_total_loss = total_loss_values[150:]\n",
    "plt.plot(filtered_total_loss)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "filtered_loss = loss_values[150:]\n",
    "plt.plot(filtered_loss)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "filtered_material_loss = material_loss_values[150:]\n",
    "plt.plot(filtered_material_loss)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Material Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "filtered_sobolev_loss = sobolev_loss_values[150:]\n",
    "plt.plot(filtered_sobolev_loss)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Sobolev Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "filtered_alpha_values = alpha_values_values[150:]\n",
    "plt.plot(filtered_alpha_values)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('alpha')\n",
    "plt.title('Alpha Values over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material_params = torch.tensor([E , A , I ], dtype=torch.float32)\n",
    "# nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "\n",
    "predicted_displacements, l2_error, energy_error, h1_error, inference_time = neural.test_portic(\n",
    "    nodes=nodes,\n",
    "    material_params=material_params,\n",
    "    model=model,\n",
    "    uh_vem=uh_vem,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    concatanate=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New geometry parameter\n",
    "I_new = 1e-4\n",
    "A_new = 2\n",
    "E_new = 110e5\n",
    "\n",
    "# Generate the geometry\n",
    "nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Solve the problem using the VEM\n",
    "uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E_new, A_new, I_new, load, q, t)\n",
    "\n",
    "# Testing the model with new parameters\n",
    "material_params = torch.tensor([E_new , A_new , I_new ], dtype=torch.float32)\n",
    "nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "predicted_displacements, l2_error, energy_error, h1_error, inference_time = neural.test_portic(\n",
    "    nodes=nodes,\n",
    "    material_params=material_params,\n",
    "    model=model,\n",
    "    uh_vem=uh_vem,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    concatanate=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New geometry parameter\n",
    "I_new_2 = 1e-4\n",
    "A_new_2 = 3\n",
    "E_new_2 = 80e3\n",
    "\n",
    "# Generate the geometry\n",
    "nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Solve the problem using the VEM\n",
    "uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E_new_2, A_new_2, I_new_2, load, q, t)\n",
    "\n",
    "# Testing the model with new parameters\n",
    "material_params = torch.tensor([E_new_2 , A_new_2 , I_new_2 ], dtype=torch.float32)\n",
    "nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "predicted_displacements, l2_error, energy_error, h1_error, inference_time = neural.test_portic(\n",
    "    nodes=nodes,\n",
    "    material_params=material_params,\n",
    "    model=model,\n",
    "    uh_vem=uh_vem,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    concatanate=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.grad_norm as gn\n",
    "import core.neural_backend as neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndof = 3 * len(nodes)\n",
    "input_dim = 2*len(nodes) + 3\n",
    "\n",
    "input_dim_nodes = 2*len(nodes)\n",
    "input_dim_materials = 3\n",
    "\n",
    "# Original material parameters\n",
    "material_params_1 = torch.tensor([E, A, I], dtype=torch.float32)\n",
    "\n",
    "# Perturbed material parameters (slightly changed)\n",
    "material_params_2 = torch.tensor([E *1.1, A * 1.1, I * 0.9], dtype=torch.float32)\n",
    "\n",
    "nodes, material_params_1 = neural.normalize_inputs(nodes, material_params_1)\n",
    "_, material_params_2 = neural.normalize_inputs(nodes, material_params_2)\n",
    "\n",
    "nodes = nodes.flatten()\n",
    "nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "input_vector = torch.cat([nodes, material_params_1])\n",
    "\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers definition\n",
    "nodes_layers = [128, 256, 512, 512, 512, 512]  # Layers for nodes sub-network\n",
    "material_layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 2048, 2048] # Layers for materials sub-network\n",
    "final_layers = [1024, 1024, 1024, 1024]  # Layers for final combination network\n",
    "\n",
    "model = neural.BeamApproximatorWithMaterials(\n",
    "                input_dim_nodes=input_dim_nodes, \n",
    "                input_dim_materials=input_dim_materials, \n",
    "                nodes_layers=nodes_layers, \n",
    "                material_layers=material_layers, \n",
    "                final_layers=final_layers, \n",
    "                ndof=ndof)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "K = torch.tensor(K, dtype=torch.float32, requires_grad=True)\n",
    "f = torch.tensor(f, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "total_loss_values = []\n",
    "loss_values = []\n",
    "material_loss_values = []\n",
    "sobolev_loss_values = []\n",
    "alpha_values_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss weights\n",
    "loss_weights = torch.ones(3, requires_grad=True)  # We have 3 tasks: loss, sobolev_loss, and material_penalty\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 200\n",
    "concatanate = False\n",
    "\n",
    "# Initialize optimizers (including the loss_weights as parameters)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + [loss_weights], lr=1e-3)\n",
    "optimizer_w = torch.optim.SGD([loss_weights], lr=1e-3)\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "total_loss_values, loss_values, material_loss_values, sobolev_loss_values, alpha_values_values = [], [], [], [], []\n",
    "\n",
    "# Enable anomaly detection for debugging\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Loop through epochs\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    uh = model(nodes, material_params_1)\n",
    "    \n",
    "    # Compute individual losses\n",
    "    loss = loss_function.compute_loss_with_uh(uh_vem, uh)\n",
    "    sobolev_loss = loss_function.compute_sobolev_loss(model, nodes, material_params_1, loss, concatanate)\n",
    "    material_penalty = loss_function.compute_material_penalty(model, nodes, material_params_1, material_params_2, concatanate) * 1e10\n",
    "\n",
    "    # Weighted sum of losses (with GradNorm weights)\n",
    "    weighted_losses = [\n",
    "        loss_weights[0] * loss, \n",
    "        loss_weights[1] * sobolev_loss, \n",
    "        loss_weights[2] * material_penalty\n",
    "    ]\n",
    "\n",
    "    # Store the initial loss weights\n",
    "    if epoch == 0:\n",
    "        initial_loss_weights = [\n",
    "            loss_weights[0] * loss, \n",
    "            loss_weights[1] * sobolev_loss, \n",
    "            loss_weights[2] * material_penalty\n",
    "        ]\n",
    "\n",
    "    # Calculate the gradient norms for each task\n",
    "    grad_norms = gn.calculate_gradient_norm(model, weighted_losses)\n",
    "    tilde_losses = [gn.compute_loss_ratio(weighted_losses[i].item(), initial_loss_weights[i].item()) for i in range(len(weighted_losses))]\n",
    "\n",
    "    # Compute the grad norm loss\n",
    "    loss_grad = gn.compute_grad_norm_loss(grad_norms, tilde_losses, 100)\n",
    "\n",
    "    # Backpropagation of the gradient loss (update the grad_loss weights)\n",
    "    loss_grad.backward(retain_graph=True)\n",
    "\n",
    "    # Step 1: Perform the optimizer step to update the task weights using the gradient loss\n",
    "    optimizer_w.step()\n",
    "\n",
    "    # Step 2: Compute the total loss (sum of the weighted loss)\n",
    "    total_loss = loss_weights[0] * loss + loss_weights[1] * sobolev_loss + loss_weights[2] * material_penalty\n",
    "\n",
    "    # Backpropagation for the model weights using total loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Step 3: Perform the optimizer step to update the model weights using the total loss\n",
    "    optimizer.step()\n",
    "\n",
    "    # Step 4: Renormalize the loss weights (no in-place operation)\n",
    "    T = len(weighted_losses)\n",
    "    sum_w = torch.sum(loss_weights).item()\n",
    "\n",
    "    # Instead of modifying in-place, re-assign to a new tensor\n",
    "    with torch.no_grad():\n",
    "        loss_weights.copy_((loss_weights / sum_w) * T)  # Use .copy_ to avoid creating new tensors and keep gradients\n",
    "\n",
    "    # Store losses for analysis\n",
    "    if epoch > 0:\n",
    "        total_loss_values.append(total_loss.item())\n",
    "        loss_values.append(loss_weights[0].item()*loss.item())\n",
    "        material_loss_values.append(loss_weights[2].item()*material_penalty.item())\n",
    "        sobolev_loss_values.append(loss_weights[1].item()*sobolev_loss.item())\n",
    "\n",
    "    # Print progress\n",
    "    print(f'Epoch: {epoch + 1}, Total Loss: {total_loss.item()}, Loss Weights: {loss_weights.detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_total_loss = total_loss_values[150:]\n",
    "plt.plot(filtered_total_loss)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "filtered_loss = loss_values[150:]\n",
    "plt.plot(filtered_loss)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "filtered_material_loss = material_loss_values[150:]\n",
    "plt.plot(filtered_material_loss)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Material Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "filtered_sobolev_loss = sobolev_loss_values[150:]\n",
    "plt.plot(filtered_sobolev_loss)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Sobolev Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "filtered_alpha_values = alpha_values_values[150:]\n",
    "plt.plot(filtered_alpha_values)\n",
    "plt.xlabel('Epochs ')\n",
    "plt.ylabel('alpha')\n",
    "plt.title('Alpha Values over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New geometry parameter\n",
    "I_new = 1e-4\n",
    "A_new = 2\n",
    "E_new = 110e5\n",
    "\n",
    "# Generate the geometry\n",
    "nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Solve the problem using the VEM\n",
    "uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E_new, A_new, I_new, load, q, t)\n",
    "\n",
    "# Testing the model with new parameters\n",
    "material_params = torch.tensor([E_new , A_new , I_new ], dtype=torch.float32)\n",
    "nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "predicted_displacements, l2_error, energy_error, h1_error, inference_time = neural.test_portic(\n",
    "    nodes=nodes,\n",
    "    material_params=material_params,\n",
    "    model=model,\n",
    "    uh_vem=uh_vem,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    concatanate=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New geometry parameter\n",
    "I_new_2 = 1e-4\n",
    "A_new_2 = 3\n",
    "E_new_2 = 80e2\n",
    "\n",
    "# Generate the geometry\n",
    "nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Solve the problem using the VEM\n",
    "uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E_new_2, A_new_2, I_new_2, load, q, t)\n",
    "\n",
    "# Testing the model with new parameters\n",
    "material_params = torch.tensor([E_new_2 , A_new_2 , I_new_2 ], dtype=torch.float32)\n",
    "nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "predicted_displacements, l2_error, energy_error, h1_error, inference_time = neural.test_portic(\n",
    "    nodes=nodes,\n",
    "    material_params=material_params,\n",
    "    model=model,\n",
    "    uh_vem=uh_vem,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    concatanate=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using few material data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.grad_norm as gn\n",
    "import core.neural_backend as neural\n",
    "from utils.helpers import generate_beam_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauloakira/Main/_repos/ai-pinn/core/neural_backend.py:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  normalized_nodes = torch.tensor(normalized_nodes, dtype=torch.float32, requires_grad=True)\n",
      "/Users/pauloakira/Main/_repos/ai-pinn/core/neural_backend.py:204: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  normalized_material_params = torch.tensor(normalized_material_params, dtype=torch.float32, requires_grad=True)\n",
      "/var/folders/r8/9jdwwqz11dq7_2m0n6cds_k40000gn/T/ipykernel_76514/2093153052.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "ndof = 3 * len(nodes)\n",
    "input_dim = 2*len(nodes) + 3\n",
    "\n",
    "input_dim_nodes = 2*len(nodes)\n",
    "input_dim_materials = 3\n",
    "\n",
    "# Original material parameters\n",
    "material_params_1 = torch.tensor([E, A, I], dtype=torch.float32)\n",
    "\n",
    "# Perturbed material parameters (slightly changed)\n",
    "material_params_2 = torch.tensor([E *1.1, A * 1.1, I * 0.9], dtype=torch.float32)\n",
    "\n",
    "nodes, material_params_1 = neural.normalize_inputs(nodes, material_params_1)\n",
    "_, material_params_2 = neural.normalize_inputs(nodes, material_params_2)\n",
    "\n",
    "nodes = nodes.flatten()\n",
    "nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "input_vector = torch.cat([nodes, material_params_1])\n",
    "\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers definition\n",
    "nodes_layers = [128, 256, 512, 512, 512, 512]  # Layers for nodes sub-network\n",
    "material_layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 2048, 2048, 4096, 4096, 2048, 2048, 1024, 1024, 1024, 1024, 512, 512] # Layers for materials sub-network\n",
    "final_layers = [1024, 1024, 1024, 1024, 1024, 1024] # Layers for final combination network\n",
    "\n",
    "model = neural.BeamApproximatorWithMaterials(\n",
    "                input_dim_nodes=input_dim_nodes, \n",
    "                input_dim_materials=input_dim_materials, \n",
    "                nodes_layers=nodes_layers, \n",
    "                material_layers=material_layers, \n",
    "                final_layers=final_layers, \n",
    "                ndof=ndof).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "K = torch.tensor(K, dtype=torch.float32, requires_grad=True).to(device)\n",
    "f = torch.tensor(f, dtype=torch.float32, requires_grad=True).to(device)\n",
    "\n",
    "total_loss_values = []\n",
    "loss_values = []\n",
    "material_loss_values = []\n",
    "sobolev_loss_values = []\n",
    "alpha_values_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalize(value, mean_val, std_val):\n",
    "    \"\"\"Z-score normalization.\"\"\"\n",
    "    return (value - mean_val) / std_val\n",
    "\n",
    "def generate_beam_dataset(elastic_module_range: list, inertia_moment_range: list, area_range: list, num_samples: int):\n",
    "    \"\"\"\n",
    "    Function to generate a dataset of beam parameters with z-score normalized material properties.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store the dataset\n",
    "    dataset = []\n",
    "\n",
    "    # Generate the material parameters\n",
    "    params = generate_beam_parameters(elastic_module_range, inertia_moment_range, area_range, num_samples)\n",
    "\n",
    "    # Extract E, I, and A values for normalization\n",
    "    E_values = [param['E'] for param in params]\n",
    "    I_values = [param['I'] for param in params]\n",
    "    A_values = [param['A'] for param in params]\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    E_mean, E_std = torch.mean(torch.tensor(E_values)), torch.std(torch.tensor(E_values))\n",
    "    I_mean, I_std = torch.mean(torch.tensor(I_values)), torch.std(torch.tensor(I_values))\n",
    "    A_mean, A_std = torch.mean(torch.tensor(A_values)), torch.std(torch.tensor(A_values))\n",
    "\n",
    "    # Normalize each parameter using z-score normalization\n",
    "    for i in range(num_samples):\n",
    "        E, I, A = params[i]['E'], params[i]['I'], params[i]['A']\n",
    "\n",
    "        # Z-score normalization\n",
    "        E_norm = z_score_normalize(E, E_mean, E_std)\n",
    "        I_norm = z_score_normalize(I, I_mean, I_std)\n",
    "        A_norm = z_score_normalize(A, A_mean, A_std)\n",
    "\n",
    "        # Generate the geometry\n",
    "        nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "        # Solve the problem using the VEM\n",
    "        uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E, A, I, load, q, t, verbose=False)\n",
    "\n",
    "        # Convert nodes to tensor\n",
    "        nodes = nodes.flatten()\n",
    "        nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        # Store the dataset\n",
    "        dataset.append({\n",
    "            \"nodes\": nodes,\n",
    "            \"elements\": elements,\n",
    "            \"supp\": supp,\n",
    "            \"load\": load,\n",
    "            \"uh_vem\": uh_vem,\n",
    "            \"K\": K,\n",
    "            \"f\": f,\n",
    "            \"material_params\": torch.tensor([E_norm, A_norm, I_norm], dtype=torch.float32),\n",
    "            \"distorted_material_params\": torch.tensor([z_score_normalize(E * 1.3, E_mean, E_std), \n",
    "                                                      z_score_normalize(A * 1.1, A_mean, A_std), \n",
    "                                                      z_score_normalize(I * 0.3, I_mean, I_std)], dtype=torch.float32)\n",
    "        })\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss weights\n",
    "loss_weights = torch.ones(3, requires_grad=True, device=device)  # We have 3 tasks: loss, sobolev_loss, and material_penalty \n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 80\n",
    "concatenate = False\n",
    "\n",
    "# Initialize optimizers (including the loss_weights as parameters)\n",
    "optimizer = torch.optim.Adam(list(model.parameters()) + [loss_weights], lr=1e-3)\n",
    "optimizer_w = torch.optim.SGD([loss_weights], lr=1e-3)\n",
    "\n",
    "# Initialize lists to store loss values\n",
    "total_loss_values, loss_values, material_loss_values, sobolev_loss_values, alpha_values_values = [], [], [], [], []\n",
    "\n",
    "# Enable anomaly detection for debugging\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Different material property configurations (for example, different E, I, A values)\n",
    "dataset = generate_beam_dataset([1e6, 210e9], [1e-6, 1e-3], [1, 10], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each material in the dataset\n",
    "for i,data in enumerate(dataset):\n",
    "    \n",
    "    material_params_1 = data['material_params']\n",
    "    material_params_2 = data['distorted_material_params']\n",
    "    nodes = data['nodes']\n",
    "    uh_vem = data['uh_vem']\n",
    "\n",
    "    nodes, material_params_1 = neural.normalize_inputs(nodes, material_params_1)\n",
    "    _, material_params_2 = neural.normalize_inputs(nodes, material_params_2)\n",
    "\n",
    "    nodes = nodes.flatten()\n",
    "    nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    # Loop through epochs for the same material\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass using the current material parameters\n",
    "        uh = model(nodes, material_params_1)  # Adjust input to include material params\n",
    "\n",
    "        # Compute individual losses\n",
    "        loss = loss_function.compute_loss_with_uh(uh_vem, uh)\n",
    "        sobolev_loss = loss_function.compute_sobolev_loss(model, nodes, material_params_1, loss, concatenate)\n",
    "        material_penalty = loss_function.compute_material_penalty(model, nodes, material_params_1, material_params_2, concatenate) \n",
    "\n",
    "        # Weighted sum of losses (with GradNorm weights)\n",
    "        weighted_losses = [\n",
    "            loss_weights[0] * loss, \n",
    "            loss_weights[1] * sobolev_loss, \n",
    "            loss_weights[2] * material_penalty\n",
    "        ]\n",
    "\n",
    "        # Store the initial loss weights\n",
    "        if epoch == 0:\n",
    "            initial_loss_weights = [\n",
    "                loss_weights[0] * loss, \n",
    "                loss_weights[1] * sobolev_loss, \n",
    "                loss_weights[2] * material_penalty\n",
    "            ]\n",
    "\n",
    "        # Calculate the gradient norms for each task\n",
    "        grad_norms = gn.calculate_gradient_norm(model, weighted_losses)\n",
    "        tilde_losses = [gn.compute_loss_ratio(weighted_losses[i].item(), initial_loss_weights[i].item()) for i in range(len(weighted_losses))]\n",
    "\n",
    "        # Compute the grad norm loss\n",
    "        loss_grad = gn.compute_grad_norm_loss(grad_norms, tilde_losses, alpha=100)\n",
    "\n",
    "        # Backpropagation of the gradient loss (update the grad_loss weights)\n",
    "        loss_grad.backward(retain_graph=True)\n",
    "\n",
    "        # Step 1: Perform the optimizer step to update the task weights using the gradient loss\n",
    "        optimizer_w.step()\n",
    "\n",
    "        # Step 2: Compute the total loss (sum of the weighted loss)\n",
    "        total_loss = loss_weights[0] * loss + loss_weights[1] * sobolev_loss + loss_weights[2] * material_penalty\n",
    "\n",
    "        # Perform gradient clipping and check for NaN/Inf before backpropagation\n",
    "        if torch.isnan(total_loss).any() or torch.isinf(total_loss).any():\n",
    "            print(f\"NaN or Inf detected in total_loss at epoch {epoch}\")\n",
    "            continue\n",
    "        \n",
    "        # Break if the total loss is too high\n",
    "        if abs(total_loss.item()) > 100.0 and epoch > 0:\n",
    "            break\n",
    "\n",
    "        # Backpropagation for the model weights using total loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Step 3: Perform the optimizer step to update the model weights using the total loss\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step 4: Renormalize the loss weights (no in-place operation)\n",
    "        T = len(weighted_losses)\n",
    "        sum_w = torch.sum(loss_weights).item()\n",
    "\n",
    "        # Instead of modifying in-place, re-assign to a new tensor\n",
    "        with torch.no_grad():\n",
    "            loss_weights.copy_((loss_weights / sum_w) * T)  # Use .copy_ to avoid creating new tensors and keep gradients\n",
    "\n",
    "        # Store losses for analysis\n",
    "        if epoch > 1:\n",
    "            total_loss_values.append(total_loss.item())\n",
    "            loss_values.append(loss_weights[0].item()*loss.item())\n",
    "            material_loss_values.append(loss_weights[2].item()*material_penalty.item())\n",
    "            sobolev_loss_values.append(loss_weights[1].item()*sobolev_loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Material {i+1}: {material_params_1}, Epoch: {epoch + 1}, Total Loss: {total_loss.item()}, Loss Weights: {loss_weights.detach().numpy()}')\n",
    "        \n",
    "    # Break if the total loss is too high\n",
    "    if abs(total_loss.item()) > 1.0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASSICAL APPROACH ##\n",
    "\n",
    "print(nodes.shape)\n",
    "print(type(nodes))\n",
    "\n",
    "# Loop through epochs (train across all materials in each epoch)\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"--> Epoch {epoch + 1}\")\n",
    "    \n",
    "    # Loop through each material in the dataset\n",
    "    for i, data in enumerate(dataset):\n",
    "        \n",
    "        # Move material parameters and nodes to the correct device\n",
    "        material_params_1 = data['material_params']\n",
    "        material_params_2 = data['distorted_material_params']\n",
    "        nodes = data['nodes']\n",
    "        uh_vem = data['uh_vem']\n",
    "\n",
    "        # Normalize inputs\n",
    "        nodes, material_params_1 = neural.normalize_inputs(nodes, material_params_1)\n",
    "        _, material_params_2 = neural.normalize_inputs(nodes, material_params_2)\n",
    "\n",
    "        material_params_1 = material_params_1.to(device)\n",
    "        material_params_2 = material_params_2.to(device)\n",
    "\n",
    "        nodes = nodes.flatten()\n",
    "        nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True).to(device)\n",
    "        \n",
    "        # Forward pass using the current material parameters\n",
    "        uh = model(nodes, material_params_1.to(device))  # Adjust input to include material params\n",
    "        uh_vem = torch.tensor(uh_vem, dtype=torch.float32, requires_grad=True).to(device)\n",
    "        # Compute individual losses\n",
    "        loss = loss_function.compute_loss_with_uh(uh_vem, uh).to(device)\n",
    "        sobolev_loss = loss_function.compute_sobolev_loss(model, nodes, material_params_1, loss, concatenate).to(device)\n",
    "        material_penalty = loss_function.compute_material_penalty(model, nodes, material_params_1, material_params_2, concatenate).to(device) * 1e10\n",
    "\n",
    "        # Weighted sum of losses (with GradNorm weights)\n",
    "        weighted_losses = [\n",
    "            loss_weights[0] * loss, \n",
    "            loss_weights[1] * sobolev_loss, \n",
    "            loss_weights[2] * material_penalty\n",
    "        ]\n",
    "\n",
    "        # Store the initial loss weights\n",
    "        if epoch == 0 and i == 0:\n",
    "            initial_loss_weights = [\n",
    "                loss_weights[0] * loss, \n",
    "                loss_weights[1] * sobolev_loss, \n",
    "                loss_weights[2] * material_penalty\n",
    "            ]\n",
    "\n",
    "        # Calculate the gradient norms for each task\n",
    "        grad_norms = gn.calculate_gradient_norm(model, weighted_losses)\n",
    "        tilde_losses = [gn.compute_loss_ratio(weighted_losses[i].item(), initial_loss_weights[i].item()) for i in range(len(weighted_losses))]\n",
    "\n",
    "        # Compute the grad norm loss\n",
    "        loss_grad = gn.compute_grad_norm_loss(grad_norms, tilde_losses, alpha=100)\n",
    "\n",
    "        # Backpropagation of the gradient loss (update the grad_loss weights)\n",
    "        loss_grad.backward(retain_graph=True)\n",
    "\n",
    "        # Step 1: Perform the optimizer step to update the task weights using the gradient loss\n",
    "        optimizer_w.step()\n",
    "\n",
    "        # Step 2: Compute the total loss (sum of the weighted loss)\n",
    "        total_loss = loss_weights[0] * loss + loss_weights[1] * sobolev_loss + loss_weights[2] * material_penalty\n",
    "\n",
    "        # Backpropagation for the model weights using total loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Step 3: Perform the optimizer step to update the model weights using the total loss\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step 4: Renormalize the loss weights (no in-place operation)\n",
    "        T = len(weighted_losses)\n",
    "        sum_w = torch.sum(loss_weights).item()\n",
    "\n",
    "        # Instead of modifying in-place, re-assign to a new tensor\n",
    "        with torch.no_grad():\n",
    "            loss_weights.copy_((loss_weights / sum_w) * T)\n",
    "\n",
    "        # Store losses for analysis\n",
    "        if epoch > 0:\n",
    "            total_loss_values.append(total_loss.item())\n",
    "            loss_values.append(loss_weights[0].item() * loss.item())\n",
    "            material_loss_values.append(loss_weights[2].item() * material_penalty.item())\n",
    "            sobolev_loss_values.append(loss_weights[1].item() * sobolev_loss.item())\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Material {i+1}: {material_params_1}, Epoch: {epoch + 1}, Total Loss: {total_loss.item()}, Loss Weights: {loss_weights.detach().cpu().numpy()}')\n",
    "\n",
    "    print(\"Finished epoch\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New geometry parameter\n",
    "I_new = 1e-4\n",
    "A_new = 1\n",
    "E_new = 110e6\n",
    "\n",
    "# Generate the geometry\n",
    "nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Solve the problem using the VEM\n",
    "uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E_new, A_new, I_new, load, q, t)\n",
    "\n",
    "# Testing the model with new parameters\n",
    "material_params = torch.tensor([E_new , A_new , I_new ], dtype=torch.float32)\n",
    "nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "predicted_displacements, l2_error, energy_error, h1_error, inference_time = neural.test_portic(\n",
    "    nodes=nodes,\n",
    "    material_params=material_params,\n",
    "    model=model,\n",
    "    uh_vem=uh_vem,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    concatanate=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"data/models/neural_vem_64.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeamApproximatorWithMaterials(\n",
       "  (nodes_in): Linear(in_features=194, out_features=128, bias=True)\n",
       "  (nodes_hidden): ModuleList(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (2-4): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (nodes_out): Linear(in_features=512, out_features=291, bias=True)\n",
       "  (materials_in): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (materials_hidden): ModuleList(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (4-5): 2 x Linear(in_features=512, out_features=512, bias=True)\n",
       "    (6): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (7-15): 9 x Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (16): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "    (17): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (18): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    (19): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (20): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (21): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (22): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (23-25): 3 x Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (26): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (27): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (materials_out): Linear(in_features=512, out_features=291, bias=True)\n",
       "  (final_in): Linear(in_features=582, out_features=1024, bias=True)\n",
       "  (final_hidden): ModuleList(\n",
       "    (0-4): 5 x Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (final_out): Linear(in_features=1024, out_features=291, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_layers = [128, 256, 512, 512, 512, 512]  # Layers for nodes sub-network\n",
    "material_layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 2048, 2048, 4096, 4096, 2048, 2048, 1024, 1024, 1024, 1024, 512, 512] # Layers for materials sub-network\n",
    "final_layers = [1024, 1024, 1024, 1024, 1024, 1024] # Layers for final combination network\n",
    "\n",
    "# Create a new instance of the model (with the same architecture)\n",
    "loaded_model = neural.BeamApproximatorWithMaterials(\n",
    "    input_dim_nodes=input_dim_nodes, \n",
    "    input_dim_materials=input_dim_materials, \n",
    "    nodes_layers=nodes_layers, \n",
    "    material_layers=material_layers, \n",
    "    final_layers=final_layers, \n",
    "    ndof=ndof\n",
    ")\n",
    "\n",
    "# Load the saved model state\n",
    "loaded_model.load_state_dict(torch.load(\"data/models/neural_vem_32.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode (important for inference)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving time [s]: 0.0095\n",
      "######################### Beam ##########################\n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.54115293e-19\n",
      " -4.78344552e-07  5.05919908e-19 -2.70328392e-19 -9.56689103e-07\n",
      " -1.21287020e-18 -3.03881964e-19 -1.43503366e-06 -2.13918497e-18\n",
      " -3.03693620e-19 -1.91337821e-06 -2.29261149e-18 -2.95807894e-19\n",
      " -2.39172276e-06 -2.38550121e-18 -3.02363445e-19 -2.87006731e-06\n",
      " -1.83049059e-18 -2.68483047e-19 -3.34841186e-06 -3.67945163e-18\n",
      " -3.92426195e-19 -3.82675641e-06  3.21986291e-18  6.46505166e-20\n",
      " -4.30510097e-06 -2.22720871e-17 -1.62507803e-18 -4.78344552e-06\n",
      "  7.19176311e-17  4.61853890e-18 -5.26179007e-06 -2.76139142e-16\n",
      " -1.84531579e-17 -5.74013462e-06  1.01000767e-15  6.68018227e-17\n",
      " -6.21847917e-06 -3.74259282e-15 -2.48234562e-16 -6.69682372e-06\n",
      "  1.38193314e-14  9.15895347e-16 -7.17516828e-06 -5.10759142e-14\n",
      " -3.38582570e-15 -7.65351283e-06  1.88726562e-13  1.25099975e-14\n",
      " -8.13185738e-06 -6.97397330e-13 -4.62286317e-14 -8.61020193e-06\n",
      "  2.57702904e-12  1.70823767e-13 -9.08854648e-06 -9.52271023e-12\n",
      " -6.31233490e-13 -9.56689103e-06  3.51885380e-11  2.33254783e-12\n",
      " -1.00452356e-05 -1.30029545e-10 -8.61928835e-12 -1.05235801e-05\n",
      "  4.80488298e-10  3.18502007e-11 -1.10019247e-05 -1.77551195e-09\n",
      " -1.17693631e-10 -1.14802692e-05  6.56091457e-09  4.34904342e-10\n",
      " -1.19586138e-05 -2.42440497e-08 -1.60706901e-09 -1.24369583e-05\n",
      "  8.95871970e-08  5.93848016e-09 -1.29153029e-05 -3.31044770e-07\n",
      " -2.19440151e-08 -1.33936474e-05  1.22328462e-06  8.10880535e-08\n",
      " -1.38719920e-05 -4.52031081e-06 -2.99638530e-07 -1.43503366e-05\n",
      "  1.67035615e-05  1.10723152e-06 -1.48286811e-05 -6.17234032e-05\n",
      " -4.09146862e-06 -1.53070257e-05  2.28081808e-04 -4.09146862e-06\n",
      " -6.12171206e-06  1.14935965e-04 -4.09146862e-06 -2.81570185e-06\n",
      "  3.84644595e-05 -4.09146862e-06 -2.50752001e-06  1.75258164e-05\n",
      " -4.09146862e-06 -2.78543715e-06  1.82296520e-05 -4.09146862e-06\n",
      " -2.90196475e-06  2.12106416e-05 -4.09146862e-06 -2.85217249e-06\n",
      "  2.20722471e-05 -4.09146862e-06 -2.74525832e-06  2.15589029e-05\n",
      " -4.09146862e-06 -2.63528272e-06  2.06905727e-05 -4.09146862e-06\n",
      " -2.53234086e-06  1.98541831e-05 -4.09146862e-06 -2.43329086e-06\n",
      "  1.90863929e-05 -4.09146862e-06 -2.33509117e-06  1.83473328e-05\n",
      " -4.09146862e-06 -2.23672564e-06  1.76120600e-05 -4.09146862e-06\n",
      " -2.13815216e-06  1.68743470e-05 -4.09146862e-06 -2.03950189e-06\n",
      "  1.61348770e-05 -4.09146862e-06 -1.94084518e-06  1.53949132e-05\n",
      " -4.09146862e-06 -1.84219678e-06  1.46549707e-05 -4.09146862e-06\n",
      " -1.74355341e-06  1.39151121e-05 -4.09146862e-06 -1.64491123e-06\n",
      "  1.31752924e-05 -4.09146862e-06 -1.54626896e-06  1.24354748e-05\n",
      " -4.09146862e-06 -1.44762618e-06  1.16956687e-05 -4.09146862e-06\n",
      " -1.34898424e-06  1.09558077e-05 -4.09146862e-06 -1.25033880e-06\n",
      "  1.02161405e-05 -4.09146862e-06 -1.15170626e-06  9.47575485e-06\n",
      " -4.09146862e-06 -1.05302610e-06  8.73802416e-06 -4.09146862e-06\n",
      " -9.54521926e-07  7.99048313e-06 -4.09146862e-06 -8.55367450e-07\n",
      "  7.27919372e-06 -4.09146862e-06 -7.58615989e-07  6.43394649e-06\n",
      " -4.09146862e-06 -6.52984846e-07  6.08370338e-06 -4.09146862e-06\n",
      " -5.80166117e-07  3.90430911e-06 -4.09146862e-06 -3.86098166e-07\n",
      "  8.48403831e-06 -4.09146862e-06 -6.40073262e-07 -1.19127085e-05\n",
      " -4.09146862e-06  7.61571109e-07  5.99842211e-05 -1.20440059e-06\n",
      "  7.37772012e-07  3.24019557e-05  2.27443575e-08  7.13972915e-07\n",
      "  6.86668255e-06  1.90932563e-07  6.90173818e-07 -1.48465998e-06\n",
      "  8.98417324e-08  6.66374721e-07 -1.75024659e-06  1.53645579e-08\n",
      "  6.42575623e-07 -6.33022991e-07 -5.91109437e-09  6.18776526e-07\n",
      " -4.77978825e-08 -5.18059179e-09  5.94977429e-07  7.11739651e-08\n",
      " -1.64725588e-09  5.71178332e-07  4.18927841e-08 -3.08999835e-11\n",
      "  5.47379235e-07  9.83060449e-09  2.30858104e-10  5.23580138e-07\n",
      " -1.45434569e-09  1.17347969e-10  4.99781040e-07 -2.17797863e-09\n",
      "  2.28354341e-11  4.75981943e-07 -8.46422479e-10 -6.36974325e-12\n",
      "  4.52182846e-07 -8.81431927e-11 -6.52001963e-12  4.28383749e-07\n",
      "  8.33343528e-11 -2.23285594e-12  4.04584652e-07  5.38548894e-11\n",
      " -1.16878395e-13  3.80785555e-07  1.38563963e-11  2.76306684e-13\n",
      "  3.56986457e-07 -1.27446949e-12  1.52378783e-13  3.33187360e-07\n",
      " -2.69121904e-12  3.31559501e-14  3.09388263e-07 -1.12390734e-12\n",
      " -6.56049805e-15  2.85589166e-07 -1.47014711e-13 -8.15428829e-15\n",
      "  2.61790069e-07  9.60177147e-14 -3.00262216e-15  2.37990972e-07\n",
      "  6.88398920e-14 -2.48957018e-16  2.14191874e-07  1.92816834e-14\n",
      "  3.26551920e-16  1.90392777e-07 -8.61106678e-16  1.96446331e-16\n",
      "  1.66593680e-07 -3.29798144e-15  4.69844895e-17  1.42794583e-07\n",
      " -1.48050677e-15 -6.57619517e-18  1.18995486e-07 -2.29144416e-16\n",
      " -1.04153394e-17  9.51963887e-08  1.10582522e-16 -4.29405849e-18\n",
      "  7.13972915e-08  8.95891853e-17 -7.28091417e-19  4.75981943e-08\n",
      "  2.88124808e-17  8.61493037e-20  2.37990972e-08  1.53394183e-18\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "#########################################################\n",
      "Predicted displacements: tensor([3.5923e-06, 4.4685e-06, 3.1423e-06, 4.6138e-06, 4.0289e-06, 4.0065e-06,\n",
      "        3.9861e-06, 4.2114e-06, 3.7244e-06, 4.2217e-06, 3.7681e-06, 4.5765e-06,\n",
      "        4.5095e-06, 4.2049e-06, 3.1032e-06, 4.0000e-06, 3.5050e-06, 4.2953e-06,\n",
      "        4.6957e-06, 4.3570e-06, 4.1593e-06, 4.3148e-06, 4.3218e-06, 4.4676e-06,\n",
      "        4.2599e-06, 4.1174e-06, 4.0252e-06, 3.6638e-06, 3.9171e-06, 3.8641e-06,\n",
      "        4.2971e-06, 5.0906e-06, 4.1965e-06, 3.9516e-06, 4.2506e-06, 3.6806e-06,\n",
      "        4.0429e-06, 3.7206e-06, 3.9414e-06, 3.9376e-06, 3.7961e-06, 3.2750e-06,\n",
      "        4.0182e-06, 3.9153e-06, 3.3677e-06, 4.6506e-06, 4.2375e-06, 4.9220e-06,\n",
      "        3.9404e-06, 4.1006e-06, 4.8857e-06, 3.7570e-06, 3.4438e-06, 4.0885e-06,\n",
      "        4.0475e-06, 4.5421e-06, 3.6992e-06, 3.7135e-06, 3.2637e-06, 3.9600e-06,\n",
      "        4.3525e-06, 3.7923e-06, 4.0613e-06, 4.3847e-06, 4.3483e-06, 4.0904e-06,\n",
      "        4.1935e-06, 4.3176e-06, 3.5521e-06, 3.1842e-06, 4.4182e-06, 3.4696e-06,\n",
      "        3.2681e-06, 3.5074e-06, 4.3958e-06, 4.3162e-06, 3.9255e-06, 3.3611e-06,\n",
      "        4.5544e-06, 3.5223e-06, 4.0159e-06, 3.2093e-06, 4.0736e-06, 3.8827e-06,\n",
      "        3.9693e-06, 4.5802e-06, 2.8657e-06, 3.3481e-06, 4.4033e-06, 4.5123e-06,\n",
      "        3.2708e-06, 4.0829e-06, 4.4829e-06, 4.5639e-06, 3.9994e-06, 4.1164e-06,\n",
      "        4.6883e-06, 3.8981e-06, 4.3542e-06, 3.9283e-06, 3.5957e-06, 3.7365e-06,\n",
      "        4.3935e-06, 3.7472e-06, 4.7875e-06, 4.2757e-06, 4.5208e-06, 4.6585e-06,\n",
      "        3.6806e-06, 4.9118e-06, 3.9618e-06, 4.0866e-06, 3.4031e-06, 3.5241e-06,\n",
      "        4.2515e-06, 3.6191e-06, 3.6987e-06, 4.5169e-06, 3.9090e-06, 4.5495e-06,\n",
      "        3.8594e-06, 3.7621e-06, 3.6889e-06, 4.0578e-06, 4.0364e-06, 3.6437e-06,\n",
      "        4.4359e-06, 4.0559e-06, 4.0748e-06, 4.1872e-06, 4.0103e-06, 3.8063e-06,\n",
      "        4.1164e-06, 4.3735e-06, 3.5483e-06, 4.4021e-06, 4.1449e-06, 3.8245e-06,\n",
      "        3.9823e-06, 4.3046e-06, 3.2140e-06, 4.1332e-06, 3.2466e-06, 4.1127e-06,\n",
      "        4.2915e-06, 4.1861e-06, 4.3642e-06, 4.2673e-06, 3.8985e-06, 4.7069e-06,\n",
      "        3.9400e-06, 4.8429e-06, 4.1593e-06, 3.2689e-06, 4.2776e-06, 3.8794e-06,\n",
      "        3.7905e-06, 4.1667e-06, 4.8527e-06, 4.1462e-06, 3.3500e-06, 5.1587e-06,\n",
      "        4.2109e-06, 3.9265e-06, 4.0177e-06, 3.4380e-06, 4.2971e-06, 4.1574e-06,\n",
      "        3.5297e-06, 4.0885e-06, 4.0606e-06, 3.6289e-06, 5.3584e-06, 3.6042e-06,\n",
      "        4.6724e-06, 3.6871e-06, 3.7616e-06, 4.0838e-06, 3.9693e-06, 4.0475e-06,\n",
      "        4.1263e-06, 3.9553e-06, 3.8203e-06, 4.2543e-06, 3.7365e-06, 3.6713e-06,\n",
      "        4.6815e-06, 4.6063e-06, 4.7144e-06, 5.0087e-06, 4.4852e-06, 3.7444e-06,\n",
      "        3.9148e-06, 5.0584e-06, 4.5327e-06, 4.2927e-06, 3.6992e-06, 4.2861e-06,\n",
      "        3.8147e-06, 4.3553e-06, 3.6205e-06, 4.2301e-06, 3.9572e-06, 3.9493e-06,\n",
      "        4.1882e-06, 3.7237e-06, 3.7355e-06, 5.1640e-06, 4.3302e-06, 3.5688e-06,\n",
      "        3.7812e-06, 4.2432e-06, 3.7486e-06, 4.0866e-06, 3.8866e-06, 3.9819e-06,\n",
      "        4.7069e-06, 4.2459e-06, 4.0131e-06, 4.2208e-06, 4.5020e-06, 3.7998e-06,\n",
      "        3.4506e-06, 4.5365e-06, 4.1155e-06, 3.6787e-06, 3.3928e-06, 3.7681e-06,\n",
      "        4.3320e-06, 3.2037e-06, 3.5930e-06, 3.9451e-06, 4.1197e-06, 4.1695e-06,\n",
      "        3.7798e-06, 4.1518e-06, 4.3605e-06, 3.9637e-06, 4.6359e-06, 4.7712e-06,\n",
      "        3.4446e-06, 4.1788e-06, 3.5539e-06, 4.2738e-06, 3.8873e-06, 4.4648e-06,\n",
      "        3.6787e-06, 3.7936e-06, 4.6855e-06, 4.2422e-06, 4.4282e-06, 4.2450e-06,\n",
      "        5.2629e-06, 3.5781e-06, 4.0978e-06, 4.0112e-06, 4.5355e-06, 3.4999e-06,\n",
      "        4.0361e-06, 4.9798e-06, 3.7160e-06, 3.9414e-06, 4.1812e-06, 4.0494e-06,\n",
      "        5.0869e-06, 3.5083e-06, 3.7774e-06, 3.8799e-06, 4.6543e-06, 4.0038e-06,\n",
      "        5.0217e-06, 3.9199e-06, 4.2142e-06, 3.4915e-06, 4.0801e-06, 3.8021e-06,\n",
      "        4.0196e-06, 4.3290e-06, 4.2282e-06, 3.8964e-06, 3.4086e-06, 4.3265e-06,\n",
      "        4.4601e-06, 3.6480e-06, 4.1588e-06, 3.3239e-06, 4.0000e-06, 3.5258e-06,\n",
      "        4.8522e-06, 3.8967e-06, 3.9069e-06], requires_grad=True)\n",
      "Inference time [s]: 0.054978132247924805\n",
      "L2 error: 1.0097533464431763\n",
      "Energy error: 0\n",
      "H1 error: 0.0002947303873952478\n"
     ]
    }
   ],
   "source": [
    "# New geometry parameters for inference\n",
    "I_new = 1e-4\n",
    "A_new = 1\n",
    "E_new = 110e6\n",
    "\n",
    "# Generate the geometry\n",
    "nodes, elements, supp, load = mesh.generate_portic_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Solve the problem using VEM\n",
    "uh_vem, K, f, solving_time = solve_vem.solve_1d(nodes, elements, supp, E_new, A_new, I_new, load, q, t)\n",
    "\n",
    "# Prepare the input for the model\n",
    "material_params = torch.tensor([E_new, A_new, I_new], dtype=torch.float32)\n",
    "nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "\n",
    "# Normalize inputs before passing to the model\n",
    "nodes, material_params = neural.normalize_inputs(nodes, material_params)\n",
    "\n",
    "# Test the model using the new material parameters\n",
    "predicted_displacements, l2_error, energy_error, h1_error, inference_time = neural.test_portic(\n",
    "    nodes=nodes,\n",
    "    material_params=material_params,\n",
    "    model=loaded_model,  # Use the loaded model for inference\n",
    "    uh_vem=uh_vem,\n",
    "    K=K,\n",
    "    f=f,\n",
    "    concatanate=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauloakira/Main/_repos/ai-pinn/core/neural_backend.py:203: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  normalized_nodes = torch.tensor(normalized_nodes, dtype=torch.float32, requires_grad=True)\n",
      "/Users/pauloakira/Main/_repos/ai-pinn/core/neural_backend.py:204: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  normalized_material_params = torch.tensor(normalized_material_params, dtype=torch.float32, requires_grad=True)\n",
      "/Users/pauloakira/Main/_repos/ai-pinn/core/neural_backend.py:400: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Material 1: tensor([-0.4345, -0.7092,  1.1438], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.22545208036899567, Loss Weights: [0.99996173 0.99996173 1.0000767 ]\n",
      "Material 2: tensor([-1.1517,  0.5040,  0.6477], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.11470791697502136, Loss Weights: [0.99990237 0.99990237 1.0001953 ]\n",
      "Material 3: tensor([ 0.4969, -1.1511,  0.6543], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.056651730090379715, Loss Weights: [0.99983 0.99983 1.00034]\n",
      "Material 4: tensor([-0.9212, -0.1423,  1.0635], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.024515947327017784, Loss Weights: [0.999751 0.999751 1.000498]\n",
      "Material 5: tensor([ 0.1277,  0.9300, -1.0577], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.009561608545482159, Loss Weights: [0.99966884 0.99966884 1.0006623 ]\n",
      "Material 6: tensor([ 0.7864,  0.3390, -1.1254], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.0063553121872246265, Loss Weights: [0.9995844 0.9995844 1.0008312]\n",
      "Material 7: tensor([ 0.3407,  0.7851, -1.1258], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.009028603322803974, Loss Weights: [0.99949896 0.99949896 1.0010021 ]\n",
      "Material 8: tensor([-0.8712, -0.2206,  1.0919], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.012635979801416397, Loss Weights: [0.999413 0.999413 1.001174]\n",
      "Material 9: tensor([ 1.1124, -0.8244, -0.2880], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.01387652475386858, Loss Weights: [0.9993272 0.9993272 1.0013458]\n",
      "Material 10: tensor([ 0.8540, -1.1001,  0.2461], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.012966458685696125, Loss Weights: [0.9992416 0.9992416 1.0015167]\n",
      "Material 11: tensor([-1.1470,  0.4585,  0.6886], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.010255549103021622, Loss Weights: [0.9991573 0.9991573 1.0016853]\n",
      "Material 12: tensor([ 1.1207, -0.8012, -0.3196], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.007767578586935997, Loss Weights: [0.99907494 0.99907494 1.0018502 ]\n",
      "Material 13: tensor([-0.1432, -0.9207,  1.0639], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.00595057662576437, Loss Weights: [0.99899465 0.99899465 1.0020107 ]\n",
      "Material 14: tensor([-0.5713,  1.1547, -0.5834], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.004952545277774334, Loss Weights: [0.99891675 0.99891675 1.0021665 ]\n",
      "Material 15: tensor([ 0.8922,  0.1888, -1.0809], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.004589950665831566, Loss Weights: [0.99884117 0.99884117 1.0023175 ]\n",
      "Material 16: tensor([-0.4727,  1.1487, -0.6760], device='mps:0', grad_fn=<ToCopyBackward0>), Epoch: 1, Total Loss: 0.004601129796355963, Loss Weights: [0.998768 0.998768 1.002464]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m material_layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m4096\u001b[39m, \u001b[38;5;241m4096\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m] \u001b[38;5;66;03m# Layers for materials sub-network\u001b[39;00m\n\u001b[1;32m      3\u001b[0m final_layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m] \u001b[38;5;66;03m# Layers for final combination network\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mneural\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_with_few_materials\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_elements_per_edge\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaterial_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaterial_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfinal_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Main/_repos/ai-pinn/core/neural_backend.py:426\u001b[0m, in \u001b[0;36mtrain_with_few_materials\u001b[0;34m(epochs, nodes, nodes_layers, material_layers, final_layers, device)\u001b[0m\n\u001b[1;32m    419\u001b[0m     initial_loss_weights \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    420\u001b[0m         loss_weights[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m loss, \n\u001b[1;32m    421\u001b[0m         loss_weights[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m sobolev_loss, \n\u001b[1;32m    422\u001b[0m         loss_weights[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m material_penalty\n\u001b[1;32m    423\u001b[0m     ]\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Calculate the gradient norms for each task\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m grad_norms \u001b[38;5;241m=\u001b[39m \u001b[43mgn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_gradient_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweighted_losses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m tilde_losses \u001b[38;5;241m=\u001b[39m [gn\u001b[38;5;241m.\u001b[39mcompute_loss_ratio(weighted_losses[i]\u001b[38;5;241m.\u001b[39mitem(), initial_loss_weights[i]\u001b[38;5;241m.\u001b[39mitem()) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weighted_losses))]\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# Compute the grad norm loss\u001b[39;00m\n",
      "File \u001b[0;32m~/Main/_repos/ai-pinn/core/grad_norm.py:33\u001b[0m, in \u001b[0;36mcalculate_gradient_norm\u001b[0;34m(model, losses)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Iterate through each task's weighted loss and calculate gradients\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m losses:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Perform the backward pass to compute the necessary gradients\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Collect the gradients of shared weights\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     shared_gradients \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-env/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/ml-env/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nodes_layers = [128, 256, 512, 512, 512, 512]  # Layers for nodes sub-network\n",
    "material_layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 2048, 2048, 4096, 4096, 2048, 2048, 1024, 1024, 1024, 1024, 512, 512] # Layers for materials sub-network\n",
    "final_layers = [1024, 1024, 1024, 1024, 1024, 1024] # Layers for final combination network\n",
    "\n",
    "neural.train_with_few_materials(\n",
    "    epochs=num_elements_per_edge,\n",
    "    nodes=nodes,\n",
    "    nodes_layers=nodes_layers,\n",
    "    material_layers=material_layers,\n",
    "    final_layers=final_layers,\n",
    "    device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
