{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points:\n",
    "\n",
    "1. **Using VEM/FEM Solutions for Efficient Training**:\n",
    "   - By training the neural network on the displacement field computed using VEM/FEM, you're providing the model with a high-quality reference solution. This allows the model to learn the underlying physical relationships between the parameters (such as Young’s modulus \\(E\\), cross-sectional area \\(A\\), and moment of inertia \\(I\\)) and the displacement field.\n",
    "\n",
    "2. **Generalization with Fewer Data**:\n",
    "   - Since the model is grounded in physically informed solutions, you likely need **fewer training examples** to generalize to new material and geometrical configurations. Unlike traditional machine learning models that require vast amounts of labeled data, your approach can rely on solving a **few instances** of VEM/FEM solutions and using that information to generalize.\n",
    "\n",
    "3. **Parameter Sensitivity and Inference**:\n",
    "   - The network’s sensitivity to material and geometrical parameters (\\(E\\), \\(A\\), \\(I\\)) is key. Once trained, the model will allow for **rapid inference** with new combinations of these parameters without needing to solve the full VEM/FEM system again.\n",
    "   - In an engineering context, this is particularly advantageous, as engineers often need to explore various material or geometric configurations during design optimization. Having a trained neural network that provides **instant predictions** without solving a full VEM/FEM problem would significantly improve efficiency.\n",
    "\n",
    "4. **Efficiency Compared to Traditional VEM/FEM**:\n",
    "   - Solving a full VEM/FEM problem repeatedly for different parameter values can be computationally expensive, especially for large or complex systems. By training a neural network to approximate the displacement field based on these parameters, you essentially create a **surrogate model** that can make predictions more efficiently.\n",
    "\n",
    "### Challenges and Considerations:\n",
    "- **Accuracy vs. Efficiency**: While the neural network may provide fast predictions, the trade-off is the potential for reduced accuracy compared to solving the full VEM/FEM system. This can be mitigated by fine-tuning the network and introducing additional regularization techniques like Sobolev training.\n",
    "  \n",
    "- **Extrapolation Limits**: The network might struggle with extrapolating far beyond the range of material and geometrical parameters it was trained on. Ensuring that the training data includes a representative range of parameters will be crucial for reliable generalization.\n",
    "\n",
    "- **Hybrid Model Validation**: You could validate your hypothesis by comparing the **computational cost** (in terms of time) and **accuracy** between solving multiple VEM/FEM instances and using the trained neural network for inference over a variety of material/geometrical configurations.\n",
    "\n",
    "### Conclusion:\n",
    "The approach of training a neural network using VEM/FEM solutions to enable efficient inference of displacement fields for different material and geometric configurations is a practical and promising solution in engineering contexts. It leverages the strengths of both numerical methods and machine learning to balance accuracy and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import core.vem as vem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS backend is available!\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS backend is available!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS backend is not available. Using CPU.\")\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2_error(uh_vem, uh_nn):\n",
    "    \"\"\"\n",
    "    Compute the L2 norm error between the FEM and NN displacement fields.\n",
    "\n",
    "    Parameters:\n",
    "    uh_fem (torch.Tensor): Displacement field from FEM (ndof x 1)\n",
    "    uh_nn (torch.Tensor): Displacement field from NN (ndof x 1)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: L2 norm error as a scalar\n",
    "    \"\"\"\n",
    "    error = torch.norm(uh_nn - uh_vem) / torch.norm(uh_vem)\n",
    "    return error\n",
    "\n",
    "def compute_energy_error(K, uh_fem, uh_nn):\n",
    "    \"\"\"\n",
    "    Compute the energy error between the FEM and NN solutions.\n",
    "\n",
    "    Parameters:\n",
    "    K (torch.Tensor): Stiffness matrix (ndof x ndof)\n",
    "    uh_fem (torch.Tensor): Displacement field from FEM (ndof x 1)\n",
    "    uh_nn (torch.Tensor): Displacement field from NN (ndof x 1)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Energy error as a scalar\n",
    "    \"\"\"\n",
    "    # Ensure all tensors are of the same type (float32)\n",
    "    K = K.float()\n",
    "    uh_fem = uh_fem.float()\n",
    "    uh_nn = uh_nn.float()\n",
    "    \n",
    "    # Compute strain energy for FEM and NN solutions\n",
    "    U_fem = 0.5 * torch.matmul(uh_fem.T, torch.matmul(K, uh_fem))\n",
    "    U_nn = 0.5 * torch.matmul(uh_nn.T, torch.matmul(K, uh_nn))\n",
    "\n",
    "    # Compute energy error\n",
    "    energy_error = (U_nn - U_fem) / U_fem\n",
    "    return energy_error.abs()  # Return the absolute value of the error\n",
    "\n",
    "import torch\n",
    "\n",
    "def compute_h1_norm(K, uh_fem, uh_nn):\n",
    "    \"\"\"\n",
    "    Compute the H1 norm between the FEM and NN solutions.\n",
    "    \n",
    "    Parameters:\n",
    "    K (torch.Tensor): Stiffness matrix (ndof x ndof)\n",
    "    uh_fem (torch.Tensor): Displacement field from FEM (ndof x 1)\n",
    "    uh_nn (torch.Tensor): Displacement field from NN (ndof x 1)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: H1 norm error as a scalar\n",
    "    \"\"\"\n",
    "    # Ensure all tensors are of the same type and have requires_grad=True\n",
    "    uh_fem = uh_fem.float().requires_grad_(True)\n",
    "    uh_nn = uh_nn.float().requires_grad_(True)\n",
    "\n",
    "    # Compute L2 norm of the displacement field\n",
    "    l2_error = torch.norm(uh_nn - uh_fem) ** 2\n",
    "\n",
    "    # Compute gradient (strain) of the displacement fields\n",
    "    grad_uh_fem = torch.autograd.grad(uh_fem.sum(), uh_fem, create_graph=True)[0]\n",
    "    grad_uh_nn = torch.autograd.grad(uh_nn.sum(), uh_nn, create_graph=True)[0]\n",
    "\n",
    "    # Compute L2 norm of the gradient (strain)\n",
    "    grad_error = torch.norm(grad_uh_nn - grad_uh_fem) ** 2\n",
    "\n",
    "    # Combine L2 norm of the displacement and gradient\n",
    "    h1_error = torch.sqrt(l2_error + grad_error)\n",
    "\n",
    "    return h1_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(K, uh, F):\n",
    "    \"\"\"\n",
    "    Compute the loss function as (K * uh - F)^2 using PyTorch\n",
    "\n",
    "    Parameters:\n",
    "    K (torch.Tensor): Stiffness matrix (ndof x ndof)\n",
    "    uh (torch.Tensor): Solution vector (ndof x 1)\n",
    "    F (torch.Tensor): Load vector (ndof x 1)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The loss value\n",
    "    \"\"\"\n",
    "    # Compute the residual\n",
    "    R = torch.matmul(K, uh) - F\n",
    "    \n",
    "    # Compute the loss (squared residual)\n",
    "    loss = torch.sum(R**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_loss_with_uh(uh_vem, uh):\n",
    "    \"\"\"\n",
    "    Compute the loss function as (uh - uh_vem)^2 using PyTorch.\n",
    "\n",
    "    Parameters:\n",
    "    uh_vem (torch.Tensor): Solution vector from VEM (ndof x 1)\n",
    "    uh (torch.Tensor): Solution vector (ndof x 1)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Detach uh_vem if necessary to avoid tracking gradients\n",
    "    uh_vem = torch.tensor(uh_vem, requires_grad=True)\n",
    "\n",
    "    # Compute the loss (squared residual)\n",
    "    loss = torch.sum((uh - uh_vem)**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_boundary_loss(uh, supp):\n",
    "    \"\"\"\n",
    "    Compute the loss function for enforcing the Dirichlet boundary conditions.\n",
    "\n",
    "    Parameters:\n",
    "    uh (torch.Tensor): Solution vector (ndof x 1)\n",
    "    supp (torch.Tensor): Support vector (ndof x N, where N is the number of nodes with boundary conditions)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The loss value as a PyTorch tensor\n",
    "    \"\"\"\n",
    "    # Initialize the loss as a scalar tensor with zero\n",
    "    loss = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    for node in supp:\n",
    "        if node[1] == 1:  # x-direction fixed\n",
    "            k = 3 * int(node[0])\n",
    "            loss = loss + uh[k] ** 2  # Avoid in-place operation by using a new tensor\n",
    "        if node[2] == 1:  # y-direction fixed\n",
    "            k = 3 * int(node[0]) + 1\n",
    "            loss = loss + uh[k] ** 2  # Avoid in-place operation by using a new tensor\n",
    "        if node[3] == 1:  # z-direction fixed\n",
    "            k = 3 * int(node[0]) + 2\n",
    "            loss = loss + uh[k] ** 2  # Avoid in-place operation by using a new tensor\n",
    "\n",
    "    return loss\n",
    "\n",
    "def compute_material_penalty(model, nodes, material_params_1, material_params_2, concatanate=False):\n",
    "    \"\"\"\n",
    "    Computes a penalty for the model if the material parameters don't affect the predictions sufficiently.\n",
    "\n",
    "    Parameters:\n",
    "    mode (str): Material penalty mode ('l1' or 'l2')\n",
    "    nodes (torch.Tensor): Node coordinates (nnodes x 3)\n",
    "    material_params_1 (torch.Tensor): Material parameters for material 1 (nnodes x 1)\n",
    "    material_params_2 (torch.Tensor): Material parameters for material 2 (nnodes x 1)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Material penalty term as a scalar\n",
    "    \"\"\"\n",
    "    if concatanate:\n",
    "        input_1 = torch.cat((nodes, material_params_1))\n",
    "        input_2 = torch.cat((nodes, material_params_2))\n",
    "\n",
    "        uh_1 = model(input_1)\n",
    "        uh_2 = model(input_2)\n",
    "    else:\n",
    "        uh_1 = model(nodes, material_params_1)\n",
    "        uh_2 = model(nodes, material_params_2)\n",
    "        \n",
    "\n",
    "    penalty = torch.sum((uh_1 - uh_2) ** 2)\n",
    "    return penalty\n",
    "\n",
    "def normalize_loss_and_penalty(loss, material_penalty, beta=1e-4):\n",
    "    \"\"\"\n",
    "    Normalizes the loss and material penalty by computing the scaling factor alpha.\n",
    "\n",
    "    A factor beta is added to the loss and penalty to avoid division by zero.\n",
    "\n",
    "    Parameters:\n",
    "    loss (torch.Tensor): Loss term as a scalar\n",
    "    material_penalty (torch.Tensor): Material penalty term as a scalar\n",
    "    beta (float): Penalty factor (default: 1e-4)\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Normalized loss term as a scalar\n",
    "    \"\"\"\n",
    "    loss_magnitude = loss.item() + beta\n",
    "    penalty_magnitude = material_penalty.item() + beta\n",
    "\n",
    "    alpha = loss_magnitude / penalty_magnitude\n",
    "    \n",
    "    return alpha\n",
    "\n",
    "\n",
    "def compute_sobolev_loss(model, nodes, material_params, displacement_loss, concatanate=False):\n",
    "    \"\"\"\n",
    "    Computes the Sobolev loss, includging both displacements and derivatives losses.\n",
    "\n",
    "    Parameters:\n",
    "    model (torch.nn.Module): Neural network model\n",
    "    nodes (torch.Tensor): Node coordinates (nnodes x 3)\n",
    "    material_params (torch.Tensor): Material parameters (nnodes x 1)\n",
    "    uh_vem (torch.Tensor): Displacement field from VEM (ndof x 1)\n",
    "    displacements_loss (torch.Tensor): Displacement loss as a scalar between uh and uh_vem\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Sobolev loss as a scalar\n",
    "    \"\"\"\n",
    "    if concatanate:\n",
    "        # Concatanete the nodes and material parameters\n",
    "        input_vector = torch.cat((nodes, material_params))\n",
    "        # Compute the displacement field from the neural network\n",
    "        uh = model(input_vector)\n",
    "    else:\n",
    "        # Compute the displacement field from the neural network\n",
    "        uh = model(nodes, material_params)\n",
    "\n",
    "    # Compute the strain (first derivative)\n",
    "    strain = torch.autograd.grad(uh, nodes, grad_outputs=torch.ones_like(uh), create_graph=True)[0]\n",
    "\n",
    "    # Compute the curvature (second derivative)\n",
    "    curvature = torch.autograd.grad(strain, nodes, grad_outputs=torch.ones_like(strain), create_graph=True)[0]\n",
    "\n",
    "    # Compute the strain loss\n",
    "    strain_loss = torch.sum(strain ** 2)\n",
    "\n",
    "    # Compute the curvature loss\n",
    "    curvature_loss = torch.sum(curvature ** 2)\n",
    "\n",
    "    # Compute the total Sobolev loss\n",
    "    sobolev_loss = displacement_loss + strain_loss + curvature_loss\n",
    "\n",
    "    return sobolev_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_geometry(num_elements_per_edge, L):\n",
    "    \n",
    "    x_coords = np.linspace(0, L, num_elements_per_edge + 1)\n",
    "    y_coords = np.linspace(0, L, num_elements_per_edge + 1)\n",
    "\n",
    "    top_nodes = np.array([[x, L] for x in x_coords if x != 0])\n",
    "    left_nodes = np.array([[0, y] for y in y_coords])\n",
    "    right_nodes = np.array([[L, y] for y in reversed(y_coords) if y != L])\n",
    "\n",
    "    nodes = np.vstack([left_nodes, top_nodes, right_nodes])\n",
    "\n",
    "    elements = np.array([[i, i+1] for i in range(len(nodes)-1)])\n",
    "    flatten_elements = elements.flatten()\n",
    "\n",
    "    supp = np.array([[flatten_elements[0], 1, 1, 1], [flatten_elements[-1], 1, 1, 1]])\n",
    "\n",
    "    return nodes, elements, supp\n",
    "\n",
    "def plot_nodes(nodes, elements):\n",
    "    \"\"\"\n",
    "    Plot the nodes and elements of a 2D mesh.\n",
    "\n",
    "    Parameters:\n",
    "    nodes (np.ndarray): Node coordinates (n_nodes x 2)\n",
    "    elements (np.ndarray): Element connectivity (n_elements x n_nodes_per_element)\n",
    "    \"\"\"\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "    \n",
    "    # Plot nodes\n",
    "    ax.plot(nodes[:, 0], nodes[:, 1], 'ro', label='Nodes')\n",
    "\n",
    "    # Plot elements as lines connecting nodes\n",
    "    for element in elements:\n",
    "        element_coords = nodes[element]\n",
    "        ax.plot(element_coords[:, 0], element_coords[:, 1], 'b-')\n",
    "\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_title('Custom Geometry Plot')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network for the beam problem\n",
    "class BeamApproximator(nn.Module):\n",
    "    def __init__(self, input_dim, layers, ndof):\n",
    "        super(BeamApproximator, self).__init__()\n",
    "        # First layer from input to the first hidden layer\n",
    "        self.fin = nn.Linear(input_dim, layers[0])\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # Final output layer from the last hidden layer to the output (ndof)\n",
    "        self.fout = nn.Linear(layers[-1], ndof)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the first layer\n",
    "        z = torch.relu(self.fin(x))\n",
    "        \n",
    "        # Pass through the hidden layers\n",
    "        for layer in self.hidden:\n",
    "            z = torch.sigmoid(layer(z))\n",
    "            # z = torch.nn.functional.leaky_relu(layer(z), negative_slope=0.01)\n",
    "        \n",
    "        # Final output layer\n",
    "        z = self.fout(z)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "class ResidualBeamApproximator(nn.Module):\n",
    "    def __init__(self, input_dim, layers, ndof):\n",
    "        super(ResidualBeamApproximator, self).__init__()\n",
    "        # First layer from input to the first hidden layer\n",
    "        self.fin = nn.Linear(input_dim, layers[0])\n",
    "        \n",
    "        # Hidden layers\n",
    "        self.hidden = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # Final output layer from the last hidden layer to the output (ndof)\n",
    "        self.fout = nn.Linear(layers[-1], ndof)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the first layer\n",
    "        z = torch.relu(self.fin(x))\n",
    "        \n",
    "        # Pass through the hidden layers\n",
    "        for layer in self.hidden:\n",
    "            z_res = z\n",
    "            z = torch.sigmoid(layer(z))\n",
    "\n",
    "            # Residual block\n",
    "            if z.shape == z_res.shape:\n",
    "                z = z + z_res\n",
    "        \n",
    "        # Final output layer\n",
    "        z = self.fout(z)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "class BeamApproximatorWithMaterials(nn.Module):\n",
    "    def __init__(self, input_dim_nodes, input_dim_materials, nodes_layers, material_layers, final_layers, ndof):\n",
    "        super(BeamApproximatorWithMaterials, self).__init__()\n",
    "        # Neural network for nodes\n",
    "        self.nodes_in = nn.Linear(input_dim_nodes, nodes_layers[0])\n",
    "        self.nodes_hidden = nn.ModuleList([nn.Linear(nodes_layers[i], nodes_layers[i+1]) for i in range(len(nodes_layers)-1)])\n",
    "        self.nodes_out = nn.Linear(nodes_layers[-1], ndof)\n",
    "\n",
    "        # Neural network for materials\n",
    "        self.materials_in = nn.Linear(input_dim_materials, material_layers[0])\n",
    "        self.materials_hidden = nn.ModuleList([nn.Linear(material_layers[i], material_layers[i+1]) for i in range(len(material_layers)-1)])\n",
    "        self.materials_out = nn.Linear(material_layers[-1], ndof)\n",
    "\n",
    "        # Final output layer\n",
    "        self.final_in = nn.Linear(ndof*2, final_layers[0])\n",
    "        self.final_hidden = nn.ModuleList([nn.Linear(final_layers[i], final_layers[i+1]) for i in range(len(final_layers)-1)])\n",
    "        self.final_out = nn.Linear(final_layers[-1], ndof)  # Output layer\n",
    "\n",
    "    def forward(self, x_nodes, x_materials):\n",
    "        # Pass through the first layer\n",
    "        z_nodes = torch.relu(self.nodes_in(x_nodes))\n",
    "        z_materials = torch.relu(self.materials_in(x_materials))\n",
    "\n",
    "        # Pass through the hidden layers\n",
    "        for layer in self.nodes_hidden:\n",
    "            z_nodes = torch.relu(layer(z_nodes))\n",
    "        z_nodes = self.nodes_out(z_nodes)\n",
    "\n",
    "        for layer in self.materials_hidden:\n",
    "            z_materials = torch.relu(layer(z_materials))\n",
    "        z_materials = self.materials_out(z_materials)\n",
    "\n",
    "        # Concatenate the nodes and materials\n",
    "        z_combined = torch.cat([z_nodes, z_materials])\n",
    "\n",
    "        # Pass through the final layers\n",
    "        z_combined = torch.relu(self.final_in(z_combined))\n",
    "        for layer in self.final_hidden:\n",
    "            z_combined = torch.relu(layer(z_combined))\n",
    "        z_combined = self.final_out(z_combined)\n",
    "        \n",
    "        return z_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################### Beam ##########################\n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -2.17339955e-08\n",
      " -2.14427330e-05  2.27723133e-07  1.12629721e-07 -4.28854659e-05\n",
      " -1.24878169e-06  2.62794616e-07 -5.91198655e-05  1.01313694e-07\n",
      " -2.80378315e-08 -6.49375985e-05  2.27919706e-06 -3.96031942e-10\n",
      " -6.55469981e-05 -2.44648029e-06 -9.34621382e-07 -6.61563978e-05\n",
      "  9.97413425e-06  4.01151099e-06 -6.67657974e-05 -4.94893421e-05\n",
      " -1.90264993e-05 -6.73751970e-05  2.33847275e-04 -1.89877264e-05\n",
      " -2.13883922e-05  1.34893551e-04 -1.89489536e-05 -3.95818289e-07\n",
      "  3.38934285e-05 -1.89101807e-05  3.21478705e-06 -4.16219740e-06\n",
      " -1.88714079e-05  1.64297824e-06 -7.56588465e-06 -1.88326351e-05\n",
      "  1.15968873e-07 -3.80380196e-06 -1.87938622e-05 -1.31417160e-07\n",
      "  2.67110206e-06 -1.87550894e-05 -1.49977127e-06 -1.27715466e-05\n",
      " -1.87163165e-05  4.87519701e-06  6.46176812e-05 -5.93255208e-06\n",
      "  4.26579739e-06  3.75985832e-05 -4.48395776e-08  3.65639776e-06\n",
      "  9.44926570e-06  9.87875854e-07  3.04699813e-06 -1.24139342e-06\n",
      "  5.53969497e-07  2.43759851e-06 -2.28370860e-06  1.44723226e-07\n",
      "  1.82819888e-06 -1.04411274e-06 -3.99003619e-09  1.21879925e-06\n",
      " -1.99444530e-07 -1.44603012e-08  6.09399627e-07  6.18312411e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "#########################################################\n"
     ]
    }
   ],
   "source": [
    "num_elements_per_edge = 8\n",
    "\n",
    "# geometry data\n",
    "L = 2.0\n",
    "I = 36e-4\n",
    "A = 0.12\n",
    "\n",
    "# material data\n",
    "E = 20e6\n",
    "\n",
    "# Generate the geometry\n",
    "nodes, elements, supp = generate_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Plot the nodes\n",
    "plot_nodes(nodes, elements)\n",
    "\n",
    "\n",
    "# loads\n",
    "load = np.array([[2,3],[3,4]])\n",
    "q = -400\n",
    "t = 0\n",
    "f_dist = vem.buildBeamDistributedLoad(load,t,q,nodes)\n",
    "\n",
    "# stiffness matrix\n",
    "K = vem.buildGlobaBeamK(nodes, elements, E, A, I, 1)\n",
    "\n",
    "# apply DBC\n",
    "K, f = vem.applyDBCBeam(K, f_dist, supp)\n",
    "\n",
    "# solve\n",
    "print()\n",
    "print(\"######################### Beam ##########################\")\n",
    "uh_vem = np.linalg.solve(K,f)\n",
    "print(uh_vem)\n",
    "print(\"#########################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model refers to the disaplcement field and the loss function regards to the calculation of the residual taking in consideration the Virtual Element Method's stiffness matrix and load vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, nodes, K, f, E, A, I, verbose=True, use_residual=False):\n",
    "    ndof = 3 * len(nodes)\n",
    "    input_dim = 2*len(nodes) + 3\n",
    "\n",
    "    nodes = nodes.flatten()\n",
    "    nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    material_params = torch.tensor([E, A, I], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    input_vector = torch.cat([nodes, material_params])\n",
    "\n",
    "    lr = 1e-3\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024]\n",
    "    # layers = [128, 128, 256, 256, 512, 512, 512, 512]\n",
    "    # layers = [128, 256, 512]\n",
    "    if use_residual:\n",
    "        model = ResidualBeamApproximator(input_dim, layers, ndof)\n",
    "    else:\n",
    "        model = BeamApproximator(input_dim, layers, ndof)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.0000000001, weight_decay=1e-4)\n",
    "    # optimizer = optim.RMSprop(model.parameters(), lr=0.0000000001)\n",
    "\n",
    "    K = torch.tensor(K, dtype=torch.float32, requires_grad=True)\n",
    "    f = torch.tensor(f, dtype=torch.float32, requires_grad=True)\n",
    "    # uh = torch.linalg.solve(K, f)\n",
    "\n",
    "    # epochs = 10000\n",
    "    loss_buffer = float('inf')  # Initialize with a large value\n",
    "    loss_values = []\n",
    "\n",
    "    # Scaling factor for loss\n",
    "    alpha = 1e-17\n",
    "\n",
    "    # Original material parameters\n",
    "    material_params_1 = torch.tensor([E, A, I], dtype=torch.float32)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        uh = model(input_vector)\n",
    "        \n",
    "        # Compute the loss\n",
    "        # loss = compute_loss_with_uh(uh_vem, uh)\n",
    "        loss = alpha * compute_loss(K, uh, f)\n",
    "        sobolev_loss = compute_sobolev_loss(model, nodes, material_params_1, loss, concatanate=True)\n",
    "        total_loss = loss + sobolev_loss\n",
    "\n",
    "        # total_loss = loss + penalty_coefficient*loss_bc\n",
    "        \n",
    "        # Only update the model if the new loss is smaller than the loss_buffer\n",
    "        \n",
    "        total_loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        loss_buffer = total_loss.item()  # Update the loss buffer with the new smaller loss\n",
    "        if epoch > 0:\n",
    "            loss_values.append(total_loss.item())\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch: {epoch + 1}, Total Loss: {total_loss.item()}')\n",
    "        \n",
    "        # Early stopping condition if the loss is not improving\n",
    "        # if total_loss.item() >= loss_buffer:\n",
    "        #     print(f'Early stopping at epoch {epoch + 1} as the loss is not improving.')\n",
    "        #     break\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training complete.\")\n",
    "        plt.plot(loss_values)\n",
    "        plt.xlabel('Epochs (Sub-Epochs)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss over Epochs')\n",
    "        plt.show()\n",
    "\n",
    "    return input_vector, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the reference displacement field calculated by the Virtual Element Method, a displacemente field is supposed to be calculated considering the material parameters contributions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_material(epochs, nodes, K, f, E, A, I, verbose=True, network_type='material'):\n",
    "    ndof = 3 * len(nodes)\n",
    "    input_dim = 2*len(nodes) + 3\n",
    "\n",
    "    input_dim_nodes = 2*len(nodes)\n",
    "    input_dim_materials = 3\n",
    "\n",
    "    nodes = nodes.flatten()\n",
    "    nodes = torch.tensor(nodes, dtype=torch.float32, requires_grad=True)\n",
    "    print(f\"Nodes shape: {nodes.shape}\")\n",
    "\n",
    "    # Original material parameters\n",
    "    material_params_1 = torch.tensor([E, A, I], dtype=torch.float32)\n",
    "    print(f\"Material params shape: {material_params_1.shape}\")\n",
    "\n",
    "    # Perturbed material parameters (slightly changed)\n",
    "    material_params_2 = torch.tensor([E *2, A * 2.2, I * 0.35], dtype=torch.float32)\n",
    "\n",
    "    input_vector = torch.cat([nodes, material_params_1])\n",
    "\n",
    "    lr = 1e-3\n",
    "\n",
    "    # Initialize the model and optimizer\n",
    "    if network_type == 'residual':\n",
    "        layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024]\n",
    "        # layers = [128, 128, 256, 256, 512, 512, 512, 512]\n",
    "        # layers = [128, 256, 512]\n",
    "        concatanate=True\n",
    "        model = ResidualBeamApproximator(input_dim, layers, ndof)\n",
    "    if network_type == 'material':\n",
    "        nodes_layers = [128, 256, 512, 512]  # Layers for nodes sub-network\n",
    "        material_layers = [128, 256, 512, 512, 1024, 1024]  # Layers for materials sub-network\n",
    "        final_layers = [1024, 1024, 1024, 1024]  # Layers for final combination network\n",
    "        # Concatanete the nodes and materials\n",
    "        concatanate = False\n",
    "        model = BeamApproximatorWithMaterials(\n",
    "            input_dim_nodes=input_dim_nodes, \n",
    "            input_dim_materials=input_dim_materials, \n",
    "            nodes_layers=nodes_layers, \n",
    "            material_layers=material_layers, \n",
    "            final_layers=final_layers, \n",
    "            ndof=ndof)\n",
    "    else:\n",
    "        layers = [128, 128, 256, 256, 512, 512, 512, 1024, 1024, 1024, 1024, 1024, 1024]\n",
    "        # layers = [128, 128, 256, 256, 512, 512, 512, 512]\n",
    "        # layers = [128, 256, 512]\n",
    "        concatanate=True\n",
    "        model = BeamApproximator(input_dim, layers, ndof)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=0.0000000001, weight_decay=1e-4)\n",
    "    # optimizer = optim.RMSprop(model.parameters(), lr=0.0000000001)\n",
    "\n",
    "    K = torch.tensor(K, dtype=torch.float32, requires_grad=True)\n",
    "    f = torch.tensor(f, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    total_loss_values = []\n",
    "    loss_values = []\n",
    "    material_loss_values = []\n",
    "    sobolev_loss_values = []\n",
    "    alpha_values_values = []\n",
    "\n",
    "    # Scaling factor for loss\n",
    "    # alpha = 1e-17\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # uh = model(input_vector)\n",
    "        uh = model(nodes, material_params_1)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = compute_loss_with_uh(uh_vem, uh)\n",
    "        # Compute the sobolev loss\n",
    "        sobolev_loss = compute_sobolev_loss(model, nodes, material_params_1,loss, concatanate)\n",
    "        # Compute material penalty\n",
    "        material_penalty = compute_material_penalty(model, nodes, material_params_1, material_params_2, concatanate)\n",
    "        # Normalize the loss and penalty\n",
    "        alpha = normalize_loss_and_penalty(loss, material_penalty)\n",
    "        total_loss = loss + alpha * material_penalty + sobolev_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if epoch > 0:\n",
    "            total_loss_values.append(total_loss.item())\n",
    "            loss_values.append(loss.item())\n",
    "            material_loss_values.append(material_penalty.item())\n",
    "            sobolev_loss_values.append(sobolev_loss.item())\n",
    "            alpha_values_values.append(alpha)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch: {epoch + 1}, Total Loss: {total_loss.item()}')\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training complete.\")\n",
    "        plt.plot(total_loss_values)\n",
    "        plt.xlabel('Epochs (Sub-Epochs)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss over Epochs')\n",
    "        plt.show()\n",
    "\n",
    "    return input_vector, model, total_loss_values, loss_values, material_loss_values, sobolev_loss_values, alpha_values_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes shape: torch.Size([50])\n",
      "Material params shape: torch.Size([3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ml-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Total Loss: 15389397.387078121\n",
      "Epoch: 2, Total Loss: 576432141.3342826\n",
      "Epoch: 3, Total Loss: 14603959.017433248\n",
      "Epoch: 4, Total Loss: 3415942.606892634\n",
      "Epoch: 5, Total Loss: 5948402.21462921\n",
      "Epoch: 6, Total Loss: 674625.2584954614\n",
      "Epoch: 7, Total Loss: 1145415.4387239276\n",
      "Epoch: 8, Total Loss: 3651434.7539753476\n",
      "Epoch: 9, Total Loss: 608441.2469097778\n",
      "Epoch: 10, Total Loss: 236968.1266601652\n",
      "Epoch: 11, Total Loss: 253357.507874313\n",
      "Epoch: 12, Total Loss: 183366.73897046974\n",
      "Epoch: 13, Total Loss: 145435.3158949911\n",
      "Epoch: 14, Total Loss: 62271.54986167097\n",
      "Epoch: 15, Total Loss: 51402.444247544685\n",
      "Epoch: 16, Total Loss: 34148.04106725127\n",
      "Epoch: 17, Total Loss: 34353.847373548735\n",
      "Epoch: 18, Total Loss: 55120.51700757908\n",
      "Epoch: 19, Total Loss: 30300.398898924585\n",
      "Epoch: 20, Total Loss: 23039.362490501993\n",
      "Epoch: 21, Total Loss: 13331.761981741567\n",
      "Epoch: 22, Total Loss: 12666.629573952807\n",
      "Epoch: 23, Total Loss: 5159.588573518707\n",
      "Epoch: 24, Total Loss: 7037.537004906616\n",
      "Epoch: 25, Total Loss: 5674.493158614914\n",
      "Epoch: 26, Total Loss: 4951.807008351287\n",
      "Epoch: 27, Total Loss: 3106.778373198651\n",
      "Epoch: 28, Total Loss: 2734.2079319828354\n",
      "Epoch: 29, Total Loss: 2495.364829903377\n",
      "Epoch: 30, Total Loss: 2283.599571192208\n",
      "Epoch: 31, Total Loss: 2208.714085016717\n",
      "Epoch: 32, Total Loss: 2319.70129589363\n",
      "Epoch: 33, Total Loss: 1924.2835841593687\n",
      "Epoch: 34, Total Loss: 1170.710316787016\n",
      "Epoch: 35, Total Loss: 1469.605178926541\n",
      "Epoch: 36, Total Loss: 776.0399649838532\n",
      "Epoch: 37, Total Loss: 949.6924644774549\n",
      "Epoch: 38, Total Loss: 556.1797990264024\n",
      "Epoch: 39, Total Loss: 492.35127696148515\n",
      "Epoch: 40, Total Loss: 534.8898096366208\n",
      "Epoch: 41, Total Loss: 461.63667975182534\n",
      "Epoch: 42, Total Loss: 277.51293006238427\n",
      "Epoch: 43, Total Loss: 222.07583101832472\n",
      "Epoch: 44, Total Loss: 275.8822581070374\n",
      "Epoch: 45, Total Loss: 186.14001494121112\n",
      "Epoch: 46, Total Loss: 190.6947812238249\n",
      "Epoch: 47, Total Loss: 139.46644128925942\n",
      "Epoch: 48, Total Loss: 162.64320073391636\n",
      "Epoch: 49, Total Loss: 164.76087734267145\n",
      "Epoch: 50, Total Loss: 118.63055966596053\n",
      "Epoch: 51, Total Loss: 114.1577027289398\n",
      "Epoch: 52, Total Loss: 91.60662593008765\n",
      "Epoch: 53, Total Loss: 67.68711651471082\n",
      "Epoch: 54, Total Loss: 70.0234031939558\n",
      "Epoch: 55, Total Loss: 53.093073573862355\n",
      "Epoch: 56, Total Loss: 72.61125516116732\n",
      "Epoch: 57, Total Loss: 52.903990647432664\n",
      "Epoch: 58, Total Loss: 44.302967942169076\n",
      "Epoch: 59, Total Loss: 93.99711882453161\n",
      "Epoch: 60, Total Loss: 63.33598713438573\n",
      "Epoch: 61, Total Loss: 57.311358706459856\n",
      "Epoch: 62, Total Loss: 44.040625571647226\n",
      "Epoch: 63, Total Loss: 45.40674232816572\n",
      "Epoch: 64, Total Loss: 32.88109946090312\n",
      "Epoch: 65, Total Loss: 24.48151142237282\n",
      "Epoch: 66, Total Loss: 30.159752606142625\n",
      "Epoch: 67, Total Loss: 24.23128715250141\n",
      "Epoch: 68, Total Loss: 21.779100388394475\n",
      "Epoch: 69, Total Loss: 23.15505813622463\n",
      "Epoch: 70, Total Loss: 14.914437968729374\n",
      "Epoch: 71, Total Loss: 18.032443079797737\n",
      "Epoch: 72, Total Loss: 15.246306260648325\n",
      "Epoch: 73, Total Loss: 20.827094925207277\n",
      "Epoch: 74, Total Loss: 11.703040949922109\n",
      "Epoch: 75, Total Loss: 10.893429646661737\n",
      "Epoch: 76, Total Loss: 8.975923572452277\n",
      "Epoch: 77, Total Loss: 9.609031691582164\n",
      "Epoch: 78, Total Loss: 8.439329197574207\n",
      "Epoch: 79, Total Loss: 9.051377449449028\n",
      "Epoch: 80, Total Loss: 5.223365059232799\n",
      "Epoch: 81, Total Loss: 5.877479419680041\n",
      "Epoch: 82, Total Loss: 4.804117410215812\n",
      "Epoch: 83, Total Loss: 5.830419749996042\n",
      "Epoch: 84, Total Loss: 5.634696576291533\n",
      "Epoch: 85, Total Loss: 3.567681023720751\n",
      "Epoch: 86, Total Loss: 3.022548544520143\n",
      "Epoch: 87, Total Loss: 2.665480404437104\n",
      "Epoch: 88, Total Loss: 3.4805762859724805\n",
      "Epoch: 89, Total Loss: 2.607252007038951\n",
      "Epoch: 90, Total Loss: 2.5906122984435367\n",
      "Epoch: 91, Total Loss: 1.314619712447919\n",
      "Epoch: 92, Total Loss: 1.5993643344996977\n",
      "Epoch: 93, Total Loss: 1.702686398800183\n",
      "Epoch: 94, Total Loss: 1.6308747645697501\n",
      "Epoch: 95, Total Loss: 1.3109722146732277\n",
      "Epoch: 96, Total Loss: 1.1674176186198153\n",
      "Epoch: 97, Total Loss: 1.1100935023351248\n",
      "Epoch: 98, Total Loss: 0.7678682301778454\n",
      "Epoch: 99, Total Loss: 1.0962201062493444\n",
      "Epoch: 100, Total Loss: 0.8682967183383117\n",
      "Epoch: 101, Total Loss: 0.802109939347492\n",
      "Epoch: 102, Total Loss: 0.6439174556216076\n",
      "Epoch: 103, Total Loss: 0.551992200144555\n",
      "Epoch: 104, Total Loss: 0.5197546190607295\n",
      "Epoch: 105, Total Loss: 0.5630577332879761\n",
      "Epoch: 106, Total Loss: 0.49918309572382114\n",
      "Epoch: 107, Total Loss: 0.2838778679638099\n",
      "Epoch: 108, Total Loss: 0.3587125358827513\n",
      "Epoch: 109, Total Loss: 0.33048308039420815\n",
      "Epoch: 110, Total Loss: 0.379879179657715\n",
      "Epoch: 111, Total Loss: 0.2886771486257493\n",
      "Epoch: 112, Total Loss: 0.23280825661155408\n",
      "Epoch: 113, Total Loss: 0.17629685922094934\n",
      "Epoch: 114, Total Loss: 0.23245515003533174\n",
      "Epoch: 115, Total Loss: 0.19384102952067594\n",
      "Epoch: 116, Total Loss: 0.16032337550489567\n",
      "Epoch: 117, Total Loss: 0.1577950312249521\n",
      "Epoch: 118, Total Loss: 0.12821083367562702\n",
      "Epoch: 119, Total Loss: 0.13112917866314772\n",
      "Epoch: 120, Total Loss: 0.12679517408180307\n",
      "Epoch: 121, Total Loss: 0.10591448840766068\n",
      "Epoch: 122, Total Loss: 0.07795904149289196\n",
      "Epoch: 123, Total Loss: 0.09608176435265336\n",
      "Epoch: 124, Total Loss: 0.06659245286680746\n",
      "Epoch: 125, Total Loss: 0.06936334833094306\n",
      "Epoch: 126, Total Loss: 0.0646238119321153\n",
      "Epoch: 127, Total Loss: 0.056864737216920105\n",
      "Epoch: 128, Total Loss: 0.04211028758892604\n",
      "Epoch: 129, Total Loss: 0.04933451605065692\n",
      "Epoch: 130, Total Loss: 0.05218568437198125\n",
      "Epoch: 131, Total Loss: 0.04323633991148183\n",
      "Epoch: 132, Total Loss: 0.04002445851950172\n",
      "Epoch: 133, Total Loss: 0.027974178354935375\n",
      "Epoch: 134, Total Loss: 0.028714169095179612\n",
      "Epoch: 135, Total Loss: 0.027527213310625825\n",
      "Epoch: 136, Total Loss: 0.02761644821791489\n",
      "Epoch: 137, Total Loss: 0.017107705516511355\n",
      "Epoch: 138, Total Loss: 0.016687602615073387\n",
      "Epoch: 139, Total Loss: 0.016443841202370725\n",
      "Epoch: 140, Total Loss: 0.013162317470185474\n",
      "Epoch: 141, Total Loss: 0.016448760245283245\n",
      "Epoch: 142, Total Loss: 0.013925635860565614\n",
      "Epoch: 143, Total Loss: 0.011627263456985261\n",
      "Epoch: 144, Total Loss: 0.008502838964853553\n",
      "Epoch: 145, Total Loss: 0.008605930937403486\n",
      "Epoch: 146, Total Loss: 0.0074818153383464266\n",
      "Epoch: 147, Total Loss: 0.00676172873761996\n",
      "Epoch: 148, Total Loss: 0.008952019261868557\n",
      "Epoch: 149, Total Loss: 0.0070047524594864866\n",
      "Epoch: 150, Total Loss: 0.005286101176037097\n",
      "Epoch: 151, Total Loss: 0.004189378933668391\n",
      "Epoch: 152, Total Loss: 0.004817846117248906\n",
      "Epoch: 153, Total Loss: 0.004784296426820451\n",
      "Epoch: 154, Total Loss: 0.0037321155572276037\n",
      "Epoch: 155, Total Loss: 0.0036891074059510432\n",
      "Epoch: 156, Total Loss: 0.002450092952959562\n",
      "Epoch: 157, Total Loss: 0.0025551101470871673\n",
      "Epoch: 158, Total Loss: 0.0029077757090064174\n",
      "Epoch: 159, Total Loss: 0.002542316130607588\n",
      "Epoch: 160, Total Loss: 0.001795345854239322\n",
      "Epoch: 161, Total Loss: 0.0013455912641143649\n",
      "Epoch: 162, Total Loss: 0.0017085913533526837\n",
      "Epoch: 163, Total Loss: 0.0014135015702507303\n",
      "Epoch: 164, Total Loss: 0.0016416184781179273\n",
      "Epoch: 165, Total Loss: 0.0014550627677414867\n",
      "Epoch: 166, Total Loss: 0.0010248158534438038\n",
      "Epoch: 167, Total Loss: 0.001032675860380629\n",
      "Epoch: 168, Total Loss: 0.000935458352469259\n",
      "Epoch: 169, Total Loss: 0.0009068887599983904\n",
      "Epoch: 170, Total Loss: 0.0007496489827589431\n",
      "Epoch: 171, Total Loss: 0.0008691644017488481\n",
      "Epoch: 172, Total Loss: 0.0006546499274943384\n",
      "Epoch: 173, Total Loss: 0.0005187870935340827\n",
      "Epoch: 174, Total Loss: 0.0005761145308258724\n",
      "Epoch: 175, Total Loss: 0.0005293347240714136\n",
      "Epoch: 176, Total Loss: 0.0005492806178500451\n",
      "Epoch: 177, Total Loss: 0.0004489226513377326\n",
      "Epoch: 178, Total Loss: 0.0004054058784478975\n",
      "Epoch: 179, Total Loss: 0.0002750074757255219\n",
      "Epoch: 180, Total Loss: 0.0003315800930995742\n",
      "Epoch: 181, Total Loss: 0.0003756746500785072\n",
      "Epoch: 182, Total Loss: 0.0003170161220643789\n",
      "Epoch: 183, Total Loss: 0.00028303194382990615\n",
      "Epoch: 184, Total Loss: 0.0002509344479303743\n",
      "Epoch: 185, Total Loss: 0.00025971417574945836\n",
      "Epoch: 186, Total Loss: 0.0002265337613315809\n",
      "Epoch: 187, Total Loss: 0.0002473343132515785\n",
      "Epoch: 188, Total Loss: 0.0002018392987676759\n",
      "Epoch: 189, Total Loss: 0.00018770731781197504\n",
      "Epoch: 190, Total Loss: 0.00018801181717782023\n",
      "Epoch: 191, Total Loss: 0.00017057021996308804\n",
      "Epoch: 192, Total Loss: 0.00017013788817561292\n",
      "Epoch: 193, Total Loss: 0.00016786893320084662\n",
      "Epoch: 194, Total Loss: 0.00016802218906818584\n",
      "Epoch: 195, Total Loss: 0.00014753968694737\n",
      "Epoch: 196, Total Loss: 0.00015647588775420104\n",
      "Epoch: 197, Total Loss: 0.00014606191520806538\n",
      "Epoch: 198, Total Loss: 0.0001460132155488346\n",
      "Epoch: 199, Total Loss: 0.00014368510406019563\n",
      "Epoch: 200, Total Loss: 0.0001336981212685571\n",
      "Epoch: 201, Total Loss: 0.00012656464753350174\n",
      "Epoch: 202, Total Loss: 0.00012801513549221344\n",
      "Epoch: 203, Total Loss: 0.00013331462295188814\n",
      "Epoch: 204, Total Loss: 0.00012265042071141324\n",
      "Epoch: 205, Total Loss: 0.00012127235126524434\n",
      "Epoch: 206, Total Loss: 0.00011779144532239063\n",
      "Epoch: 207, Total Loss: 0.00011879127722830738\n",
      "Epoch: 208, Total Loss: 0.00011684083142120825\n",
      "Epoch: 209, Total Loss: 0.00011654535523128893\n",
      "Epoch: 210, Total Loss: 0.00011280065218378048\n",
      "Epoch: 211, Total Loss: 0.00011255414552952491\n",
      "Epoch: 212, Total Loss: 0.00011467733216192043\n",
      "Epoch: 213, Total Loss: 0.000111935577866713\n",
      "Epoch: 214, Total Loss: 0.00011137024594591678\n",
      "Epoch: 215, Total Loss: 0.00010947659945478741\n",
      "Epoch: 216, Total Loss: 0.00010936266365561918\n",
      "Epoch: 217, Total Loss: 0.00010862456817651352\n",
      "Epoch: 218, Total Loss: 0.00010923450634652386\n",
      "Epoch: 219, Total Loss: 0.00010752287776471584\n",
      "Epoch: 220, Total Loss: 0.00010757227034451149\n",
      "Epoch: 221, Total Loss: 0.00010762014601746199\n",
      "Epoch: 222, Total Loss: 0.00010660777818139312\n",
      "Epoch: 223, Total Loss: 0.00010638515488127335\n",
      "Epoch: 224, Total Loss: 0.00010640919939011831\n",
      "Epoch: 225, Total Loss: 0.00010622400595279921\n",
      "Epoch: 226, Total Loss: 0.00010537305298668811\n",
      "Epoch: 227, Total Loss: 0.0001054434713502577\n",
      "Epoch: 228, Total Loss: 0.000105267237215597\n",
      "Epoch: 229, Total Loss: 0.00010570319256620204\n",
      "Epoch: 230, Total Loss: 0.00010588163590354678\n",
      "Epoch: 231, Total Loss: 0.0001055117283066253\n",
      "Epoch: 232, Total Loss: 0.0001048516763284423\n",
      "Epoch: 233, Total Loss: 0.0001047001333392743\n",
      "Epoch: 234, Total Loss: 0.000104881292441687\n",
      "Epoch: 235, Total Loss: 0.00010482998198425928\n",
      "Epoch: 236, Total Loss: 0.00010486253974953372\n",
      "Epoch: 237, Total Loss: 0.00010469228563095311\n",
      "Epoch: 238, Total Loss: 0.00010462397235246423\n",
      "Epoch: 239, Total Loss: 0.00010453727603560676\n",
      "Epoch: 240, Total Loss: 0.00010460314333195212\n",
      "Epoch: 241, Total Loss: 0.00010451716262465548\n",
      "Epoch: 242, Total Loss: 0.0001045260502616045\n",
      "Epoch: 243, Total Loss: 0.00010451352901921616\n",
      "Epoch: 244, Total Loss: 0.000104350044871284\n",
      "Epoch: 245, Total Loss: 0.00010441928468716958\n",
      "Epoch: 246, Total Loss: 0.00010430128958438168\n",
      "Epoch: 247, Total Loss: 0.00010433307661681634\n",
      "Epoch: 248, Total Loss: 0.0001043192050718332\n",
      "Epoch: 249, Total Loss: 0.0001043441584556133\n",
      "Epoch: 250, Total Loss: 0.00010429310021667189\n",
      "Epoch: 251, Total Loss: 0.00010416101225552236\n",
      "Epoch: 252, Total Loss: 0.00010415922904250427\n",
      "Epoch: 253, Total Loss: 0.00010409142349501177\n",
      "Epoch: 254, Total Loss: 0.00010416542423913054\n",
      "Epoch: 255, Total Loss: 0.00010412441503433815\n",
      "Epoch: 256, Total Loss: 0.00010414872755245278\n",
      "Epoch: 257, Total Loss: 0.00010412064644139192\n",
      "Epoch: 258, Total Loss: 0.000104128261734292\n",
      "Epoch: 259, Total Loss: 0.00010412207684748709\n",
      "Epoch: 260, Total Loss: 0.00010407368349727166\n",
      "Epoch: 261, Total Loss: 0.00010408052264383982\n",
      "Epoch: 262, Total Loss: 0.00010410697530273424\n",
      "Epoch: 263, Total Loss: 0.00010413099443750987\n",
      "Epoch: 264, Total Loss: 0.00010409562326632512\n",
      "Epoch: 265, Total Loss: 0.00010404139124298064\n",
      "Epoch: 266, Total Loss: 0.00010402852180257623\n",
      "Epoch: 267, Total Loss: 0.00010405204039990796\n",
      "Epoch: 268, Total Loss: 0.00010410837950448944\n",
      "Epoch: 269, Total Loss: 0.00010406905978157274\n",
      "Epoch: 270, Total Loss: 0.00010405076293355388\n",
      "Epoch: 271, Total Loss: 0.00010403068294030573\n",
      "Epoch: 272, Total Loss: 0.00010402450841541896\n",
      "Epoch: 273, Total Loss: 0.00010405873581994355\n",
      "Epoch: 274, Total Loss: 0.00010404618845650637\n",
      "Epoch: 275, Total Loss: 0.00010404594417994263\n",
      "Epoch: 276, Total Loss: 0.00010404944512505215\n",
      "Epoch: 277, Total Loss: 0.00010405292449337371\n",
      "Epoch: 278, Total Loss: 0.00010406551940343172\n",
      "Epoch: 279, Total Loss: 0.00010403758310898606\n",
      "Epoch: 280, Total Loss: 0.00010402947923387063\n",
      "Epoch: 281, Total Loss: 0.00010402915756359531\n",
      "Epoch: 282, Total Loss: 0.00010402999686043258\n",
      "Epoch: 283, Total Loss: 0.00010403883279963864\n",
      "Epoch: 284, Total Loss: 0.00010405668250536049\n",
      "Epoch: 285, Total Loss: 0.0001040207960813757\n",
      "Epoch: 286, Total Loss: 0.00010401498965743988\n",
      "Epoch: 287, Total Loss: 0.00010401963383134335\n",
      "Epoch: 288, Total Loss: 0.00010403819711779603\n",
      "Epoch: 289, Total Loss: 0.00010405271093609386\n",
      "Epoch: 290, Total Loss: 0.00010404169646166943\n",
      "Epoch: 291, Total Loss: 0.0001040134863886039\n",
      "Epoch: 292, Total Loss: 0.00010401255091229395\n",
      "Epoch: 293, Total Loss: 0.00010399893745584836\n",
      "Epoch: 294, Total Loss: 0.00010401691576570083\n",
      "Epoch: 295, Total Loss: 0.00010403232569541714\n",
      "Epoch: 296, Total Loss: 0.00010402859509204754\n",
      "Epoch: 297, Total Loss: 0.00010400863453391261\n",
      "Epoch: 298, Total Loss: 0.00010402555233260714\n",
      "Epoch: 299, Total Loss: 0.00010403656373055915\n",
      "Epoch: 300, Total Loss: 0.00010403731268745386\n",
      "Epoch: 301, Total Loss: 0.0001040311381721768\n",
      "Epoch: 302, Total Loss: 0.00010401707691844544\n",
      "Epoch: 303, Total Loss: 0.00010401131241551104\n",
      "Epoch: 304, Total Loss: 0.0001040260761728131\n",
      "Epoch: 305, Total Loss: 0.00010403159117803714\n",
      "Epoch: 306, Total Loss: 0.00010402614525283744\n",
      "Epoch: 307, Total Loss: 0.00010400938725163678\n",
      "Epoch: 308, Total Loss: 0.00010401575032039945\n",
      "Epoch: 309, Total Loss: 0.0001040201417387868\n",
      "Epoch: 310, Total Loss: 0.00010403924088299738\n",
      "Epoch: 311, Total Loss: 0.00010404626349413111\n",
      "Epoch: 312, Total Loss: 0.00010404520027029884\n",
      "Epoch: 313, Total Loss: 0.00010402045810874416\n",
      "Epoch: 314, Total Loss: 0.00010402428036243187\n",
      "Epoch: 315, Total Loss: 0.00010401317082667368\n",
      "Epoch: 316, Total Loss: 0.00010402218550688552\n",
      "Epoch: 317, Total Loss: 0.00010401891439552411\n",
      "Epoch: 318, Total Loss: 0.00010402099322722897\n",
      "Epoch: 319, Total Loss: 0.00010402369431296708\n",
      "Epoch: 320, Total Loss: 0.00010402756403501788\n",
      "Epoch: 321, Total Loss: 0.0001040204355286679\n",
      "Epoch: 322, Total Loss: 0.00010401247611360791\n",
      "Epoch: 323, Total Loss: 0.00010400007323741581\n",
      "Epoch: 324, Total Loss: 0.00010400546001215402\n",
      "Epoch: 325, Total Loss: 0.00010402185909938062\n",
      "Epoch: 326, Total Loss: 0.00010404347581609256\n",
      "Epoch: 327, Total Loss: 0.00010403760690258444\n",
      "Epoch: 328, Total Loss: 0.000104020738547649\n",
      "Epoch: 329, Total Loss: 0.00010401191823521407\n",
      "Epoch: 330, Total Loss: 0.00010400488251302231\n",
      "Epoch: 331, Total Loss: 0.00010401020876687751\n",
      "Epoch: 332, Total Loss: 0.00010401730365739584\n",
      "Epoch: 333, Total Loss: 0.00010401174761081754\n",
      "Epoch: 334, Total Loss: 0.00010403411909272708\n",
      "Epoch: 335, Total Loss: 0.00010402269155348294\n",
      "Epoch: 336, Total Loss: 0.00010403050221361013\n",
      "Epoch: 337, Total Loss: 0.00010400416195728058\n",
      "Epoch: 338, Total Loss: 0.00010398668941735638\n",
      "Epoch: 339, Total Loss: 0.00010400156645431978\n",
      "Epoch: 340, Total Loss: 0.00010402529394535386\n",
      "Epoch: 341, Total Loss: 0.00010405172293554294\n",
      "Epoch: 342, Total Loss: 0.00010402543033765705\n",
      "Epoch: 343, Total Loss: 0.0001040133302583041\n",
      "Epoch: 344, Total Loss: 0.00010400813945848613\n",
      "Epoch: 345, Total Loss: 0.00010401253292091164\n",
      "Epoch: 346, Total Loss: 0.00010401562578183805\n",
      "Epoch: 347, Total Loss: 0.00010401229837716974\n",
      "Epoch: 348, Total Loss: 0.00010400123341276654\n",
      "Epoch: 349, Total Loss: 0.00010399560875435225\n",
      "Epoch: 350, Total Loss: 0.00010401179621828262\n",
      "Epoch: 351, Total Loss: 0.00010402147353032274\n",
      "Epoch: 352, Total Loss: 0.00010400247799845691\n",
      "Epoch: 353, Total Loss: 0.0001039961809136208\n",
      "Epoch: 354, Total Loss: 0.00010400629871495974\n",
      "Epoch: 355, Total Loss: 0.00010403541922774482\n",
      "Epoch: 356, Total Loss: 0.00010400735622421385\n",
      "Epoch: 357, Total Loss: 0.00010400328922041639\n",
      "Epoch: 358, Total Loss: 0.0001039926733157621\n",
      "Epoch: 359, Total Loss: 0.00010398752795582982\n",
      "Epoch: 360, Total Loss: 0.00010397721999797453\n",
      "Epoch: 361, Total Loss: 0.00010400381980145646\n",
      "Epoch: 362, Total Loss: 0.00010402933439972\n",
      "Epoch: 363, Total Loss: 0.00010400766356515734\n",
      "Epoch: 364, Total Loss: 0.00010398847708102918\n",
      "Epoch: 365, Total Loss: 0.00010399533117281837\n",
      "Epoch: 366, Total Loss: 0.00010399429899201232\n",
      "Epoch: 367, Total Loss: 0.00010398654803937758\n",
      "Epoch: 368, Total Loss: 0.00010399344631674675\n",
      "Epoch: 369, Total Loss: 0.0001040118330493017\n",
      "Epoch: 370, Total Loss: 0.00010401124759370332\n",
      "Epoch: 371, Total Loss: 0.00010399015437431394\n",
      "Epoch: 372, Total Loss: 0.0001039773669750625\n",
      "Epoch: 373, Total Loss: 0.00010399055852749927\n",
      "Epoch: 374, Total Loss: 0.0001040157462365866\n",
      "Epoch: 375, Total Loss: 0.00010399471651659505\n",
      "Epoch: 376, Total Loss: 0.00010401240060883618\n",
      "Epoch: 377, Total Loss: 0.00010398099587595342\n",
      "Epoch: 378, Total Loss: 0.00010398781152930211\n",
      "Epoch: 379, Total Loss: 0.00010398401715739681\n",
      "Epoch: 380, Total Loss: 0.00010402231344216865\n",
      "Epoch: 381, Total Loss: 0.00010401054955764039\n",
      "Epoch: 382, Total Loss: 0.000103981163432788\n",
      "Epoch: 383, Total Loss: 0.00010399314858023159\n",
      "Epoch: 384, Total Loss: 0.00010400050539589867\n",
      "Epoch: 385, Total Loss: 0.00010400228035845535\n",
      "Epoch: 386, Total Loss: 0.00010399182640541871\n",
      "Epoch: 387, Total Loss: 0.00010398421219094144\n",
      "Epoch: 388, Total Loss: 0.00010398902008620169\n",
      "Epoch: 389, Total Loss: 0.00010398547451757661\n",
      "Epoch: 390, Total Loss: 0.00010400232717674209\n",
      "Epoch: 391, Total Loss: 0.00010401399051363069\n",
      "Epoch: 392, Total Loss: 0.00010397240241408879\n",
      "Epoch: 393, Total Loss: 0.00010398532788444398\n",
      "Epoch: 394, Total Loss: 0.00010399514792650374\n",
      "Epoch: 395, Total Loss: 0.00010399370506486774\n",
      "Epoch: 396, Total Loss: 0.00010397447408763274\n",
      "Epoch: 397, Total Loss: 0.00010398460993691399\n",
      "Epoch: 398, Total Loss: 0.00010396874091313059\n",
      "Epoch: 399, Total Loss: 0.00010402025244285899\n",
      "Epoch: 400, Total Loss: 0.00010400132835108343\n",
      "Epoch: 401, Total Loss: 0.00010399291055553697\n",
      "Epoch: 402, Total Loss: 0.00010397166361235261\n",
      "Epoch: 403, Total Loss: 0.00010396557855375067\n",
      "Epoch: 404, Total Loss: 0.00010400546275990416\n",
      "Epoch: 405, Total Loss: 0.00010398582249501788\n",
      "Epoch: 406, Total Loss: 0.00010398221980667261\n",
      "Epoch: 407, Total Loss: 0.00010399195267296368\n",
      "Epoch: 408, Total Loss: 0.00010402109733805457\n",
      "Epoch: 409, Total Loss: 0.00010401339808012975\n",
      "Epoch: 410, Total Loss: 0.00010405482637757617\n",
      "Epoch: 411, Total Loss: 0.00010397744632685403\n",
      "Epoch: 412, Total Loss: 0.00010409124970156618\n",
      "Epoch: 413, Total Loss: 0.00010396596939437678\n",
      "Epoch: 414, Total Loss: 0.00010402395572010483\n",
      "Epoch: 415, Total Loss: 0.00010398915130330772\n",
      "Epoch: 416, Total Loss: 0.00010408787612317671\n",
      "Epoch: 417, Total Loss: 0.00010395891937997314\n",
      "Epoch: 418, Total Loss: 0.00010399328041871562\n",
      "Epoch: 419, Total Loss: 0.00010398262437276602\n",
      "Epoch: 420, Total Loss: 0.00010404853623769076\n",
      "Epoch: 421, Total Loss: 0.0001039904355025713\n",
      "Epoch: 422, Total Loss: 0.00010398736521353741\n",
      "Epoch: 423, Total Loss: 0.00010397496318318954\n",
      "Epoch: 424, Total Loss: 0.00010409420908777091\n",
      "Epoch: 425, Total Loss: 0.00010400178742441001\n",
      "Epoch: 426, Total Loss: 0.00010409178496911849\n",
      "Epoch: 427, Total Loss: 0.00010397648371990398\n",
      "Epoch: 428, Total Loss: 0.00010417652042368195\n",
      "Epoch: 429, Total Loss: 0.0001039855274660111\n",
      "Epoch: 430, Total Loss: 0.00010406050955091546\n",
      "Epoch: 431, Total Loss: 0.00010401146986051374\n",
      "Epoch: 432, Total Loss: 0.00010404040053353799\n",
      "Epoch: 433, Total Loss: 0.00010400273806018161\n",
      "Epoch: 434, Total Loss: 0.00010402219395132413\n",
      "Epoch: 435, Total Loss: 0.00010409114889288004\n",
      "Epoch: 436, Total Loss: 0.00010399249235578726\n",
      "Epoch: 437, Total Loss: 0.00010397859434388895\n",
      "Epoch: 438, Total Loss: 0.00010395673842061978\n",
      "Epoch: 439, Total Loss: 0.00010404441652171233\n",
      "Epoch: 440, Total Loss: 0.00010397822525670444\n",
      "Epoch: 441, Total Loss: 0.00010405581754295733\n",
      "Epoch: 442, Total Loss: 0.00010395368291672778\n",
      "Epoch: 443, Total Loss: 0.00010403785850720166\n",
      "Epoch: 444, Total Loss: 0.00010394214929972746\n",
      "Epoch: 445, Total Loss: 0.00010396044883551669\n",
      "Epoch: 446, Total Loss: 0.0001040365577469429\n",
      "Epoch: 447, Total Loss: 0.00010401980988687638\n",
      "Epoch: 448, Total Loss: 0.00010396093416538009\n",
      "Epoch: 449, Total Loss: 0.00010393757088145126\n",
      "Epoch: 450, Total Loss: 0.00010395357783986542\n",
      "Epoch: 451, Total Loss: 0.0001040038720037545\n",
      "Epoch: 452, Total Loss: 0.00010399344411098462\n",
      "Epoch: 453, Total Loss: 0.00010393773146929341\n",
      "Epoch: 454, Total Loss: 0.00010395034928615285\n",
      "Epoch: 455, Total Loss: 0.00010399414333885139\n",
      "Epoch: 456, Total Loss: 0.00010395304137775596\n",
      "Epoch: 457, Total Loss: 0.0001039797172865622\n",
      "Epoch: 458, Total Loss: 0.00010396365931111167\n",
      "Epoch: 459, Total Loss: 0.00010401001674589295\n",
      "Epoch: 460, Total Loss: 0.00010393077077354565\n",
      "Epoch: 461, Total Loss: 0.00010393963344527854\n",
      "Epoch: 462, Total Loss: 0.00010396536093324884\n",
      "Epoch: 463, Total Loss: 0.00010404893866254798\n",
      "Epoch: 464, Total Loss: 0.00010394706412892557\n",
      "Epoch: 465, Total Loss: 0.000103945138456318\n",
      "Epoch: 466, Total Loss: 0.00010394638689699705\n",
      "Epoch: 467, Total Loss: 0.00010400222794187432\n",
      "Epoch: 468, Total Loss: 0.00010397611839123565\n",
      "Epoch: 469, Total Loss: 0.00010395197482919636\n",
      "Epoch: 470, Total Loss: 0.00010396209980153198\n",
      "Epoch: 471, Total Loss: 0.00010396788692695191\n",
      "Epoch: 472, Total Loss: 0.00010396087310841087\n",
      "Epoch: 473, Total Loss: 0.00010395890562212061\n",
      "Epoch: 474, Total Loss: 0.00010396581628021559\n",
      "Epoch: 475, Total Loss: 0.00010396244986934211\n",
      "Epoch: 476, Total Loss: 0.00010393112856088707\n",
      "Epoch: 477, Total Loss: 0.0001039283179028693\n",
      "Epoch: 478, Total Loss: 0.00010397181567471607\n",
      "Epoch: 479, Total Loss: 0.00010399952813702895\n",
      "Epoch: 480, Total Loss: 0.00010393994808861025\n",
      "Epoch: 481, Total Loss: 0.00010393935287340017\n",
      "Epoch: 482, Total Loss: 0.00010399939012119603\n",
      "Epoch: 483, Total Loss: 0.00010397129836610869\n",
      "Epoch: 484, Total Loss: 0.00010400589833955561\n",
      "Epoch: 485, Total Loss: 0.00010395738200107268\n",
      "Epoch: 486, Total Loss: 0.00010408633130705414\n",
      "Epoch: 487, Total Loss: 0.00010392040440359397\n",
      "Epoch: 488, Total Loss: 0.00010394325583854369\n",
      "Epoch: 489, Total Loss: 0.00010400807093974247\n",
      "Epoch: 490, Total Loss: 0.00010399406228929717\n",
      "Epoch: 491, Total Loss: 0.00010395945527789436\n",
      "Epoch: 492, Total Loss: 0.00010393491751019949\n",
      "Epoch: 493, Total Loss: 0.0001039896313234928\n",
      "Epoch: 494, Total Loss: 0.00010394329090532587\n",
      "Epoch: 495, Total Loss: 0.00010395994418459348\n",
      "Epoch: 496, Total Loss: 0.00010393565099980941\n",
      "Epoch: 497, Total Loss: 0.00010400500638400821\n",
      "Epoch: 498, Total Loss: 0.00010393271839946984\n",
      "Epoch: 499, Total Loss: 0.00010395363797849745\n",
      "Epoch: 500, Total Loss: 0.0001039390807455636\n",
      "Epoch: 501, Total Loss: 0.0001039737661056961\n",
      "Epoch: 502, Total Loss: 0.00010392324847278276\n",
      "Epoch: 503, Total Loss: 0.0001039370271404017\n",
      "Epoch: 504, Total Loss: 0.00010397816674551858\n",
      "Epoch: 505, Total Loss: 0.00010395735995408431\n",
      "Epoch: 506, Total Loss: 0.00010390640967786478\n",
      "Epoch: 507, Total Loss: 0.00010391127373118354\n",
      "Epoch: 508, Total Loss: 0.00010396189569311372\n",
      "Epoch: 509, Total Loss: 0.00010398870986393446\n",
      "Epoch: 510, Total Loss: 0.00010395768564564942\n",
      "Epoch: 511, Total Loss: 0.0001039056841531572\n",
      "Epoch: 512, Total Loss: 0.00010394650725426368\n",
      "Epoch: 513, Total Loss: 0.00010391767037322495\n",
      "Epoch: 514, Total Loss: 0.00010394635491657304\n",
      "Epoch: 515, Total Loss: 0.00010393529556221531\n",
      "Epoch: 516, Total Loss: 0.00010393187824584731\n",
      "Epoch: 517, Total Loss: 0.0001039249131905512\n",
      "Epoch: 518, Total Loss: 0.00010391847473462554\n",
      "Epoch: 519, Total Loss: 0.00010394193672178239\n",
      "Epoch: 520, Total Loss: 0.00010395679914754568\n",
      "Epoch: 521, Total Loss: 0.00010391798848705951\n",
      "Epoch: 522, Total Loss: 0.00010393480395571431\n",
      "Epoch: 523, Total Loss: 0.00010390245921611314\n",
      "Epoch: 524, Total Loss: 0.0001040446953845843\n",
      "Epoch: 525, Total Loss: 0.00010397170139006288\n",
      "Epoch: 526, Total Loss: 0.00010393742490717766\n",
      "Epoch: 527, Total Loss: 0.00010407071844531298\n",
      "Epoch: 528, Total Loss: 0.00010394251804823925\n",
      "Epoch: 529, Total Loss: 0.00010395592915571347\n",
      "Epoch: 530, Total Loss: 0.00010397644123246596\n",
      "Epoch: 531, Total Loss: 0.00010393523201785376\n",
      "Epoch: 532, Total Loss: 0.00010392141303754921\n",
      "Epoch: 533, Total Loss: 0.00010390456311706921\n",
      "Epoch: 534, Total Loss: 0.00010395446342626634\n",
      "Epoch: 535, Total Loss: 0.00010395854994950599\n",
      "Epoch: 536, Total Loss: 0.00010392673673895289\n",
      "Epoch: 537, Total Loss: 0.00010390952136343\n",
      "Epoch: 538, Total Loss: 0.00010392258474500621\n",
      "Epoch: 539, Total Loss: 0.00010394588617192648\n",
      "Epoch: 540, Total Loss: 0.00010394975948056846\n",
      "Epoch: 541, Total Loss: 0.00010395427271684297\n",
      "Epoch: 542, Total Loss: 0.00010389560964543156\n",
      "Epoch: 543, Total Loss: 0.0001039083591192509\n",
      "Epoch: 544, Total Loss: 0.00010394197827211016\n",
      "Epoch: 545, Total Loss: 0.00010397037899652874\n",
      "Epoch: 546, Total Loss: 0.00010391061818354147\n",
      "Epoch: 547, Total Loss: 0.00010392601599325615\n",
      "Epoch: 548, Total Loss: 0.00010393617772116187\n",
      "Epoch: 549, Total Loss: 0.00010391669582563481\n",
      "Epoch: 550, Total Loss: 0.00010396153903752356\n",
      "Epoch: 551, Total Loss: 0.00010391615194534141\n",
      "Epoch: 552, Total Loss: 0.00010391303816009991\n",
      "Epoch: 553, Total Loss: 0.00010389636869572613\n",
      "Epoch: 554, Total Loss: 0.00010391562490270505\n",
      "Epoch: 555, Total Loss: 0.00010392396747242603\n",
      "Epoch: 556, Total Loss: 0.00010391576079990308\n",
      "Epoch: 557, Total Loss: 0.0001038980743190659\n",
      "Epoch: 558, Total Loss: 0.00010391068579303523\n",
      "Epoch: 559, Total Loss: 0.00010391724271242771\n",
      "Epoch: 560, Total Loss: 0.00010394063597551486\n",
      "Epoch: 561, Total Loss: 0.00010391266759312765\n",
      "Epoch: 562, Total Loss: 0.00010391992148068649\n",
      "Epoch: 563, Total Loss: 0.00010389905744631045\n",
      "Epoch: 564, Total Loss: 0.00010391690137049011\n",
      "Epoch: 565, Total Loss: 0.00010390762638715735\n",
      "Epoch: 566, Total Loss: 0.00010392266556484038\n",
      "Epoch: 567, Total Loss: 0.00010390012001776158\n",
      "Epoch: 568, Total Loss: 0.00010391882977007827\n",
      "Epoch: 569, Total Loss: 0.00010391212836075172\n",
      "Epoch: 570, Total Loss: 0.00010392536976535887\n",
      "Epoch: 571, Total Loss: 0.00010391274396800057\n",
      "Epoch: 572, Total Loss: 0.00010388690904071502\n",
      "Epoch: 573, Total Loss: 0.0001039122591297674\n",
      "Epoch: 574, Total Loss: 0.00010397729159587814\n",
      "Epoch: 575, Total Loss: 0.00010393318157864243\n",
      "Epoch: 576, Total Loss: 0.00010389173593184246\n",
      "Epoch: 577, Total Loss: 0.00010391822646043796\n",
      "Epoch: 578, Total Loss: 0.00010395109763365948\n",
      "Epoch: 579, Total Loss: 0.00010392356100962364\n",
      "Epoch: 580, Total Loss: 0.00010389231084756732\n",
      "Epoch: 581, Total Loss: 0.00010391982278760135\n",
      "Epoch: 582, Total Loss: 0.00010387621009102076\n",
      "Epoch: 583, Total Loss: 0.00010390336828198802\n",
      "Epoch: 584, Total Loss: 0.00010394030985876965\n",
      "Epoch: 585, Total Loss: 0.00010389217614812933\n",
      "Epoch: 586, Total Loss: 0.00010387837690411413\n",
      "Epoch: 587, Total Loss: 0.00010387836132478491\n",
      "Epoch: 588, Total Loss: 0.00010393422444350512\n",
      "Epoch: 589, Total Loss: 0.00010391264900363789\n",
      "Epoch: 590, Total Loss: 0.00010389029116697354\n",
      "Epoch: 591, Total Loss: 0.00010387680665731102\n",
      "Epoch: 592, Total Loss: 0.00010388975549457466\n",
      "Epoch: 593, Total Loss: 0.0001039204955809945\n",
      "Epoch: 594, Total Loss: 0.00010390832668967544\n",
      "Epoch: 595, Total Loss: 0.0001038929190132364\n",
      "Epoch: 596, Total Loss: 0.00010389677606609772\n",
      "Epoch: 597, Total Loss: 0.00010389552426384683\n",
      "Epoch: 598, Total Loss: 0.00010389196513435934\n",
      "Epoch: 599, Total Loss: 0.00010389145541838573\n",
      "Epoch: 600, Total Loss: 0.00010387251847159435\n",
      "Epoch: 601, Total Loss: 0.00010388487876737497\n",
      "Epoch: 602, Total Loss: 0.00010387266867746623\n",
      "Epoch: 603, Total Loss: 0.00010389181926725375\n",
      "Epoch: 604, Total Loss: 0.00010388850480826784\n",
      "Epoch: 605, Total Loss: 0.00010387620476436378\n",
      "Epoch: 606, Total Loss: 0.00010387373740260562\n",
      "Epoch: 607, Total Loss: 0.00010389355594830874\n",
      "Epoch: 608, Total Loss: 0.00010387501213671369\n",
      "Epoch: 609, Total Loss: 0.00010386967087839006\n",
      "Epoch: 610, Total Loss: 0.00010388575312832019\n",
      "Epoch: 611, Total Loss: 0.0001038870036666061\n",
      "Epoch: 612, Total Loss: 0.0001038882273980571\n",
      "Epoch: 613, Total Loss: 0.00010386988637407836\n",
      "Epoch: 614, Total Loss: 0.00010388677318764125\n",
      "Epoch: 615, Total Loss: 0.00010390634769823413\n",
      "Epoch: 616, Total Loss: 0.00010386368155358816\n",
      "Epoch: 617, Total Loss: 0.00010386342777406115\n",
      "Epoch: 618, Total Loss: 0.00010387887408091939\n",
      "Epoch: 619, Total Loss: 0.00010388022851744145\n",
      "Epoch: 620, Total Loss: 0.00010388468075105828\n",
      "Epoch: 621, Total Loss: 0.0001038854995740554\n",
      "Epoch: 622, Total Loss: 0.00010386221952892756\n",
      "Epoch: 623, Total Loss: 0.00010388270667974987\n",
      "Epoch: 624, Total Loss: 0.00010387994384463659\n",
      "Epoch: 625, Total Loss: 0.00010387247146107157\n",
      "Epoch: 626, Total Loss: 0.00010386029933821163\n",
      "Epoch: 627, Total Loss: 0.0001038837902803653\n",
      "Epoch: 628, Total Loss: 0.0001038745520310298\n",
      "Epoch: 629, Total Loss: 0.0001038800845578849\n",
      "Epoch: 630, Total Loss: 0.0001038857008289228\n",
      "Epoch: 631, Total Loss: 0.000103847798768367\n",
      "Epoch: 632, Total Loss: 0.0001038600994208503\n",
      "Epoch: 633, Total Loss: 0.00010391987045722914\n",
      "Epoch: 634, Total Loss: 0.00010388451164046917\n",
      "Epoch: 635, Total Loss: 0.00010384697232532862\n",
      "Epoch: 636, Total Loss: 0.00010389783720297219\n",
      "Epoch: 637, Total Loss: 0.00010390021929225062\n",
      "Epoch: 638, Total Loss: 0.00010385245042773684\n",
      "Epoch: 639, Total Loss: 0.00010386309369346528\n",
      "Epoch: 640, Total Loss: 0.00010388358971332939\n",
      "Epoch: 641, Total Loss: 0.00010388366966270149\n",
      "Epoch: 642, Total Loss: 0.00010387880960664332\n",
      "Epoch: 643, Total Loss: 0.00010390231216956717\n",
      "Epoch: 644, Total Loss: 0.00010383810105477959\n",
      "Epoch: 645, Total Loss: 0.00010384605793277055\n",
      "Epoch: 646, Total Loss: 0.00010389108644534752\n",
      "Epoch: 647, Total Loss: 0.00010386928101543651\n",
      "Epoch: 648, Total Loss: 0.00010385354821018558\n",
      "Epoch: 649, Total Loss: 0.00010388389929440168\n",
      "Epoch: 650, Total Loss: 0.00010385864322372406\n",
      "Epoch: 651, Total Loss: 0.00010388367484443566\n",
      "Epoch: 652, Total Loss: 0.00010389843069429194\n",
      "Epoch: 653, Total Loss: 0.00010388885091171937\n",
      "Epoch: 654, Total Loss: 0.0001038498621358203\n",
      "Epoch: 655, Total Loss: 0.00010383640750686655\n",
      "Epoch: 656, Total Loss: 0.00010382278201312325\n",
      "Epoch: 657, Total Loss: 0.00010391878356986702\n",
      "Epoch: 658, Total Loss: 0.00010384532390853672\n",
      "Epoch: 659, Total Loss: 0.00010383320728367393\n",
      "Epoch: 660, Total Loss: 0.00010385417333967284\n",
      "Epoch: 661, Total Loss: 0.00010387984018067408\n",
      "Epoch: 662, Total Loss: 0.00010385463500683283\n",
      "Epoch: 663, Total Loss: 0.00010384858896814135\n",
      "Epoch: 664, Total Loss: 0.00010388506641566771\n",
      "Epoch: 665, Total Loss: 0.00010383772682421436\n",
      "Epoch: 666, Total Loss: 0.00010385344023664401\n",
      "Epoch: 667, Total Loss: 0.00010394717628123634\n",
      "Epoch: 668, Total Loss: 0.00010389043746539549\n",
      "Epoch: 669, Total Loss: 0.00010383900712737849\n",
      "Epoch: 670, Total Loss: 0.00010384467682649038\n",
      "Epoch: 671, Total Loss: 0.00010384913693356417\n",
      "Epoch: 672, Total Loss: 0.00010385032593229537\n",
      "Epoch: 673, Total Loss: 0.00010382967037487678\n",
      "Epoch: 674, Total Loss: 0.0001038296270591273\n",
      "Epoch: 675, Total Loss: 0.00010385994480054041\n",
      "Epoch: 676, Total Loss: 0.0001038459072619898\n",
      "Epoch: 677, Total Loss: 0.00010385308085603608\n",
      "Epoch: 678, Total Loss: 0.00010382148584328114\n",
      "Epoch: 679, Total Loss: 0.00010384461466109287\n",
      "Epoch: 680, Total Loss: 0.00010382061493311755\n",
      "Epoch: 681, Total Loss: 0.0001038359717206983\n",
      "Epoch: 682, Total Loss: 0.0001038632270954072\n",
      "Epoch: 683, Total Loss: 0.0001038375031643098\n",
      "Epoch: 684, Total Loss: 0.0001038423699837378\n",
      "Epoch: 685, Total Loss: 0.00010382834736525118\n",
      "Epoch: 686, Total Loss: 0.00010382751721268977\n",
      "Epoch: 687, Total Loss: 0.00010383567757029627\n",
      "Epoch: 688, Total Loss: 0.00010386591740120613\n",
      "Epoch: 689, Total Loss: 0.00010386503996672247\n",
      "Epoch: 690, Total Loss: 0.00010380873687470627\n",
      "Epoch: 691, Total Loss: 0.00010381405233683783\n",
      "Epoch: 692, Total Loss: 0.00010389090579269233\n",
      "Epoch: 693, Total Loss: 0.00010385217251232404\n",
      "Epoch: 694, Total Loss: 0.00010380922809095843\n",
      "Epoch: 695, Total Loss: 0.00010384343675082165\n",
      "Epoch: 696, Total Loss: 0.00010385816727162935\n",
      "Epoch: 697, Total Loss: 0.00010381269971064874\n",
      "Epoch: 698, Total Loss: 0.00010386370380940113\n",
      "Epoch: 699, Total Loss: 0.00010383921896813306\n",
      "Epoch: 700, Total Loss: 0.00010383515229582128\n",
      "Epoch: 701, Total Loss: 0.00010388216505114093\n",
      "Epoch: 702, Total Loss: 0.0001038274265967706\n",
      "Epoch: 703, Total Loss: 0.00010382056848383807\n",
      "Epoch: 704, Total Loss: 0.00010387934757168934\n",
      "Epoch: 705, Total Loss: 0.0001038080441595558\n",
      "Epoch: 706, Total Loss: 0.00010384224675323247\n",
      "Epoch: 707, Total Loss: 0.00010384897811730346\n",
      "Epoch: 708, Total Loss: 0.00010382687896792755\n",
      "Epoch: 709, Total Loss: 0.00010383533907324277\n",
      "Epoch: 710, Total Loss: 0.00010381244725965076\n",
      "Epoch: 711, Total Loss: 0.00010383105467727183\n",
      "Epoch: 712, Total Loss: 0.00010383686728592878\n",
      "Epoch: 713, Total Loss: 0.0001038193506023168\n",
      "Epoch: 714, Total Loss: 0.00010382947897076447\n",
      "Epoch: 715, Total Loss: 0.0001038051834700478\n",
      "Epoch: 716, Total Loss: 0.00010381736805909205\n",
      "Epoch: 717, Total Loss: 0.00010387875620239604\n",
      "Epoch: 718, Total Loss: 0.00010378742364788632\n",
      "Epoch: 719, Total Loss: 0.00010379945308292631\n",
      "Epoch: 720, Total Loss: 0.00010380812434247088\n",
      "Epoch: 721, Total Loss: 0.00010382954522253677\n",
      "Epoch: 722, Total Loss: 0.0001038163125355987\n",
      "Epoch: 723, Total Loss: 0.00010384103160954419\n",
      "Epoch: 724, Total Loss: 0.00010381530371740273\n",
      "Epoch: 725, Total Loss: 0.00010378214676035451\n",
      "Epoch: 726, Total Loss: 0.00010380231468530118\n",
      "Epoch: 727, Total Loss: 0.00010382718498000573\n",
      "Epoch: 728, Total Loss: 0.00010379612515306764\n",
      "Epoch: 729, Total Loss: 0.00010379212548315239\n",
      "Epoch: 730, Total Loss: 0.00010382814152950468\n",
      "Epoch: 731, Total Loss: 0.00010382666604939387\n",
      "Epoch: 732, Total Loss: 0.00010380519749524612\n",
      "Epoch: 733, Total Loss: 0.00010380708716374351\n",
      "Epoch: 734, Total Loss: 0.00010380995694750597\n",
      "Epoch: 735, Total Loss: 0.00010379952611514297\n",
      "Epoch: 736, Total Loss: 0.00010378667215643831\n",
      "Epoch: 737, Total Loss: 0.00010382482457738093\n",
      "Epoch: 738, Total Loss: 0.00010380665982101471\n",
      "Epoch: 739, Total Loss: 0.0001037881877915233\n",
      "Epoch: 740, Total Loss: 0.0001038052227424995\n",
      "Epoch: 741, Total Loss: 0.0001038035016494033\n",
      "Epoch: 742, Total Loss: 0.00010381229646861368\n",
      "Epoch: 743, Total Loss: 0.0001037994731662997\n",
      "Epoch: 744, Total Loss: 0.00010379948262271337\n",
      "Epoch: 745, Total Loss: 0.00010379582038276978\n",
      "Epoch: 746, Total Loss: 0.00010379408857564713\n",
      "Epoch: 747, Total Loss: 0.00010385305207874534\n",
      "Epoch: 748, Total Loss: 0.0001038563834830555\n",
      "Epoch: 749, Total Loss: 0.00010379853342244396\n",
      "Epoch: 750, Total Loss: 0.0001038039340897309\n",
      "Epoch: 751, Total Loss: 0.00010386756917704856\n",
      "Epoch: 752, Total Loss: 0.00010392080424947563\n",
      "Epoch: 753, Total Loss: 0.00010379160594964405\n",
      "Epoch: 754, Total Loss: 0.0001037807196254006\n",
      "Epoch: 755, Total Loss: 0.00010393968846674848\n",
      "Epoch: 756, Total Loss: 0.00010390137924736343\n",
      "Epoch: 757, Total Loss: 0.00010388604367451548\n",
      "Epoch: 758, Total Loss: 0.00010377394811335956\n",
      "Epoch: 759, Total Loss: 0.00010378971026947026\n",
      "Epoch: 760, Total Loss: 0.00010382039101049321\n",
      "Epoch: 761, Total Loss: 0.0001038063415283693\n",
      "Epoch: 762, Total Loss: 0.0001037810340558745\n",
      "Epoch: 763, Total Loss: 0.00010383233831432205\n",
      "Epoch: 764, Total Loss: 0.00010377389069593981\n",
      "Epoch: 765, Total Loss: 0.00010378662601539222\n",
      "Epoch: 766, Total Loss: 0.00010390303527934495\n",
      "Epoch: 767, Total Loss: 0.00010379069976960761\n",
      "Epoch: 768, Total Loss: 0.00010376398532512538\n",
      "Epoch: 769, Total Loss: 0.00010385313371615343\n",
      "Epoch: 770, Total Loss: 0.00010377854092687882\n",
      "Epoch: 771, Total Loss: 0.00010376448671817678\n",
      "Epoch: 772, Total Loss: 0.00010381567107314664\n",
      "Epoch: 773, Total Loss: 0.0001038073055521257\n",
      "Epoch: 774, Total Loss: 0.0001037910406720165\n",
      "Epoch: 775, Total Loss: 0.00010382465420953556\n",
      "Epoch: 776, Total Loss: 0.00010376866374874516\n",
      "Epoch: 777, Total Loss: 0.00010377587052904725\n",
      "Epoch: 778, Total Loss: 0.00010392755261608947\n",
      "Epoch: 779, Total Loss: 0.00010387874307039597\n",
      "Epoch: 780, Total Loss: 0.00010383392993437143\n",
      "Epoch: 781, Total Loss: 0.00010377553620987112\n",
      "Epoch: 782, Total Loss: 0.00010388220911514197\n",
      "Epoch: 783, Total Loss: 0.0001040405531689094\n",
      "Epoch: 784, Total Loss: 0.0001038975743573266\n",
      "Epoch: 785, Total Loss: 0.00010381167003804432\n",
      "Epoch: 786, Total Loss: 0.000103775442628638\n",
      "Epoch: 787, Total Loss: 0.00010386190637114918\n",
      "Epoch: 788, Total Loss: 0.0001038966059655045\n",
      "Epoch: 789, Total Loss: 0.00010381084777618755\n",
      "Epoch: 790, Total Loss: 0.0001037646166022835\n",
      "Epoch: 791, Total Loss: 0.00010382402517810301\n",
      "Epoch: 792, Total Loss: 0.00010386126758346695\n",
      "Epoch: 793, Total Loss: 0.00010379849630954874\n",
      "Epoch: 794, Total Loss: 0.00010375709080076502\n",
      "Epoch: 795, Total Loss: 0.00010382324416643529\n",
      "Epoch: 796, Total Loss: 0.0001037919361817273\n",
      "Epoch: 797, Total Loss: 0.00010376886545966903\n",
      "Epoch: 798, Total Loss: 0.00010379554551492786\n",
      "Epoch: 799, Total Loss: 0.00010377583562818675\n",
      "Epoch: 800, Total Loss: 0.00010379047306473046\n",
      "Epoch: 801, Total Loss: 0.00010381387692339714\n",
      "Epoch: 802, Total Loss: 0.00010375091153773324\n",
      "Epoch: 803, Total Loss: 0.00010374473367552904\n",
      "Epoch: 804, Total Loss: 0.00010377318822446123\n",
      "Epoch: 805, Total Loss: 0.00010377286054593607\n",
      "Epoch: 806, Total Loss: 0.0001037611495875247\n",
      "Epoch: 807, Total Loss: 0.00010375956637630724\n",
      "Epoch: 808, Total Loss: 0.00010374486887105197\n",
      "Epoch: 809, Total Loss: 0.00010377894298934208\n",
      "Epoch: 810, Total Loss: 0.00010378109641469879\n",
      "Epoch: 811, Total Loss: 0.00010374183738012986\n",
      "Epoch: 812, Total Loss: 0.00010373613785688587\n",
      "Epoch: 813, Total Loss: 0.00010381843526732407\n",
      "Epoch: 814, Total Loss: 0.00010375359447110843\n",
      "Epoch: 815, Total Loss: 0.00010374821418821658\n",
      "Epoch: 816, Total Loss: 0.00010379685946066948\n",
      "Epoch: 817, Total Loss: 0.00010380766441314335\n",
      "Epoch: 818, Total Loss: 0.00010392874326980184\n",
      "Epoch: 819, Total Loss: 0.00010377091237056377\n",
      "Epoch: 820, Total Loss: 0.00010376386641340967\n",
      "Epoch: 821, Total Loss: 0.00010382139669429629\n",
      "Epoch: 822, Total Loss: 0.00010377716990084287\n",
      "Epoch: 823, Total Loss: 0.00010384201979292805\n",
      "Epoch: 824, Total Loss: 0.0001038340379004864\n",
      "Epoch: 825, Total Loss: 0.00010391920478906684\n",
      "Epoch: 826, Total Loss: 0.00010381984783147042\n",
      "Epoch: 827, Total Loss: 0.0001037743386930299\n",
      "Epoch: 828, Total Loss: 0.00010384079097167361\n",
      "Epoch: 829, Total Loss: 0.00010386573572888235\n",
      "Epoch: 830, Total Loss: 0.00010394934798592154\n",
      "Epoch: 831, Total Loss: 0.00010390329234097938\n",
      "Epoch: 832, Total Loss: 0.00010394307880643892\n",
      "Epoch: 833, Total Loss: 0.00010380149290845712\n",
      "Epoch: 834, Total Loss: 0.00010389816502032442\n",
      "Epoch: 835, Total Loss: 0.0001038267122602845\n",
      "Epoch: 836, Total Loss: 0.00010378011033786597\n",
      "Epoch: 837, Total Loss: 0.00010386464489010663\n",
      "Epoch: 838, Total Loss: 0.00010388241347099981\n",
      "Epoch: 839, Total Loss: 0.00010404221802644994\n",
      "Epoch: 840, Total Loss: 0.0001038464214846189\n",
      "Epoch: 841, Total Loss: 0.00010385511044620641\n",
      "Epoch: 842, Total Loss: 0.00010373169432977757\n",
      "Epoch: 843, Total Loss: 0.00010373381410953606\n",
      "Epoch: 844, Total Loss: 0.00010386585343387985\n",
      "Epoch: 845, Total Loss: 0.0001038289542503253\n",
      "Epoch: 846, Total Loss: 0.00010401808048808963\n",
      "Epoch: 847, Total Loss: 0.0001038452749656088\n",
      "Epoch: 848, Total Loss: 0.00010390376366274263\n",
      "Epoch: 849, Total Loss: 0.00010371885070788265\n",
      "Epoch: 850, Total Loss: 0.00010370617737219507\n",
      "Epoch: 851, Total Loss: 0.00010383228375717603\n",
      "Epoch: 852, Total Loss: 0.00010381273230235892\n",
      "Epoch: 853, Total Loss: 0.00010398212841061\n",
      "Epoch: 854, Total Loss: 0.00010381447932889459\n",
      "Epoch: 855, Total Loss: 0.00010392364991723089\n",
      "Epoch: 856, Total Loss: 0.00010388483076185782\n",
      "Epoch: 857, Total Loss: 0.00010394773836215106\n",
      "Epoch: 858, Total Loss: 0.000103771146842822\n",
      "Epoch: 859, Total Loss: 0.00010378612843081367\n",
      "Epoch: 860, Total Loss: 0.00010377914604243332\n",
      "Epoch: 861, Total Loss: 0.0001038351030705418\n",
      "Epoch: 862, Total Loss: 0.00010422051993926287\n",
      "Epoch: 863, Total Loss: 0.00010456874219206332\n",
      "Epoch: 864, Total Loss: 0.00010614347388152585\n",
      "Epoch: 865, Total Loss: 0.0001098907563368375\n",
      "Epoch: 866, Total Loss: 0.00012619656067643174\n",
      "Epoch: 867, Total Loss: 0.00018708961340944296\n",
      "Epoch: 868, Total Loss: 0.0004525854264253262\n",
      "Epoch: 869, Total Loss: 0.0015634240547641688\n",
      "Epoch: 870, Total Loss: 0.006799690403570235\n",
      "Epoch: 871, Total Loss: 0.033057955366832883\n",
      "Epoch: 872, Total Loss: 0.260677650791341\n",
      "Epoch: 873, Total Loss: 3.849765108079124\n",
      "Epoch: 874, Total Loss: 76.47973881866133\n",
      "Epoch: 875, Total Loss: 1421.4206995591503\n",
      "Epoch: 876, Total Loss: 8730.536993137957\n",
      "Epoch: 877, Total Loss: 12171.055336924817\n",
      "Epoch: 878, Total Loss: 101941.04450442668\n",
      "Epoch: 879, Total Loss: 403130.3910607182\n",
      "Epoch: 880, Total Loss: 1637524.720598084\n",
      "Epoch: 881, Total Loss: 6821990.218287248\n",
      "Epoch: 882, Total Loss: 497408.7408887722\n",
      "Epoch: 883, Total Loss: 788309.9051020648\n",
      "Epoch: 884, Total Loss: 725408.0370903237\n",
      "Epoch: 885, Total Loss: 263176.46924339014\n",
      "Epoch: 886, Total Loss: 51926.64920610846\n",
      "Epoch: 887, Total Loss: 40832.104331871335\n",
      "Epoch: 888, Total Loss: 30670.490303988845\n",
      "Epoch: 889, Total Loss: 52417.96116377265\n",
      "Epoch: 890, Total Loss: 17649.959781739693\n",
      "Epoch: 891, Total Loss: 6932.373289589397\n",
      "Epoch: 892, Total Loss: 30946.075819364094\n",
      "Epoch: 893, Total Loss: 32853.37491206367\n",
      "Epoch: 894, Total Loss: 24559.853561770524\n",
      "Epoch: 895, Total Loss: 23368.281447857516\n",
      "Epoch: 896, Total Loss: 26559.48229177895\n",
      "Epoch: 897, Total Loss: 5400.343964541003\n",
      "Epoch: 898, Total Loss: 6195.697941151614\n",
      "Epoch: 899, Total Loss: 6017.275147819575\n",
      "Epoch: 900, Total Loss: 4709.129302828676\n",
      "Epoch: 901, Total Loss: 3728.1875250637695\n",
      "Epoch: 902, Total Loss: 1399.487225850537\n",
      "Epoch: 903, Total Loss: 2966.266682096015\n",
      "Epoch: 904, Total Loss: 2146.408007493471\n",
      "Epoch: 905, Total Loss: 1042.457916470557\n",
      "Epoch: 906, Total Loss: 777.4721988643355\n",
      "Epoch: 907, Total Loss: 708.1547693306211\n",
      "Epoch: 908, Total Loss: 246.39681585510846\n",
      "Epoch: 909, Total Loss: 231.65005619883857\n",
      "Epoch: 910, Total Loss: 170.36207709121678\n",
      "Epoch: 911, Total Loss: 386.8901665862304\n",
      "Epoch: 912, Total Loss: 333.0667361700996\n",
      "Epoch: 913, Total Loss: 160.28665850343228\n",
      "Epoch: 914, Total Loss: 97.30588314659663\n",
      "Epoch: 915, Total Loss: 103.42289947288344\n",
      "Epoch: 916, Total Loss: 64.79240401586497\n",
      "Epoch: 917, Total Loss: 28.283206774975113\n",
      "Epoch: 918, Total Loss: 36.090551447538665\n",
      "Epoch: 919, Total Loss: 37.526988569399116\n",
      "Epoch: 920, Total Loss: 26.322191818716718\n",
      "Epoch: 921, Total Loss: 30.54641950067466\n",
      "Epoch: 922, Total Loss: 15.968043772578444\n",
      "Epoch: 923, Total Loss: 17.11168858414927\n",
      "Epoch: 924, Total Loss: 15.728967718179664\n",
      "Epoch: 925, Total Loss: 12.501753475166616\n",
      "Epoch: 926, Total Loss: 8.045047309064595\n",
      "Epoch: 927, Total Loss: 7.466150861587884\n",
      "Epoch: 928, Total Loss: 7.842359932455487\n",
      "Epoch: 929, Total Loss: 5.097746456581348\n",
      "Epoch: 930, Total Loss: 6.896586871674028\n",
      "Epoch: 931, Total Loss: 5.230334126481257\n",
      "Epoch: 932, Total Loss: 5.1716603517126885\n",
      "Epoch: 933, Total Loss: 3.9734814894893375\n",
      "Epoch: 934, Total Loss: 3.8775200620550105\n",
      "Epoch: 935, Total Loss: 2.788882682900188\n",
      "Epoch: 936, Total Loss: 2.4764437783595463\n",
      "Epoch: 937, Total Loss: 2.070242572108673\n",
      "Epoch: 938, Total Loss: 1.6646264205594399\n",
      "Epoch: 939, Total Loss: 1.5753097989113047\n",
      "Epoch: 940, Total Loss: 1.5250714743485503\n",
      "Epoch: 941, Total Loss: 1.5069513235925127\n",
      "Epoch: 942, Total Loss: 1.2318015031526626\n",
      "Epoch: 943, Total Loss: 1.1546469177488576\n",
      "Epoch: 944, Total Loss: 0.9603240960335615\n",
      "Epoch: 945, Total Loss: 0.7932174534416379\n",
      "Epoch: 946, Total Loss: 0.629700155543922\n",
      "Epoch: 947, Total Loss: 0.6734830976627322\n",
      "Epoch: 948, Total Loss: 0.6888738540606658\n",
      "Epoch: 949, Total Loss: 0.6322531583952017\n",
      "Epoch: 950, Total Loss: 0.599418611491424\n",
      "Epoch: 951, Total Loss: 0.5492017227180629\n",
      "Epoch: 952, Total Loss: 0.4695256600855511\n",
      "Epoch: 953, Total Loss: 0.3778450687787523\n",
      "Epoch: 954, Total Loss: 0.35967793739364584\n",
      "Epoch: 955, Total Loss: 0.36657492375478334\n",
      "Epoch: 956, Total Loss: 0.32944522325864045\n",
      "Epoch: 957, Total Loss: 0.28313803542333327\n",
      "Epoch: 958, Total Loss: 0.2788239399785199\n",
      "Epoch: 959, Total Loss: 0.2619461386782145\n",
      "Epoch: 960, Total Loss: 0.20295712650716258\n",
      "Epoch: 961, Total Loss: 0.17097953571873414\n",
      "Epoch: 962, Total Loss: 0.1483051973070588\n",
      "Epoch: 963, Total Loss: 0.1399047570249521\n",
      "Epoch: 964, Total Loss: 0.1280276520944422\n",
      "Epoch: 965, Total Loss: 0.11571341882496913\n",
      "Epoch: 966, Total Loss: 0.11318238350309906\n",
      "Epoch: 967, Total Loss: 0.09590330003578056\n",
      "Epoch: 968, Total Loss: 0.08443633024253058\n",
      "Epoch: 969, Total Loss: 0.07731292149014683\n",
      "Epoch: 970, Total Loss: 0.0782435569451368\n",
      "Epoch: 971, Total Loss: 0.0792244748851867\n",
      "Epoch: 972, Total Loss: 0.07344997953337014\n",
      "Epoch: 973, Total Loss: 0.06391855809571759\n",
      "Epoch: 974, Total Loss: 0.054287041197685126\n",
      "Epoch: 975, Total Loss: 0.048803085414734604\n",
      "Epoch: 976, Total Loss: 0.040546235550412604\n",
      "Epoch: 977, Total Loss: 0.03469992776274672\n",
      "Epoch: 978, Total Loss: 0.029725898605015733\n",
      "Epoch: 979, Total Loss: 0.028688947299156074\n",
      "Epoch: 980, Total Loss: 0.027788344014147898\n",
      "Epoch: 981, Total Loss: 0.023792564171300448\n",
      "Epoch: 982, Total Loss: 0.02272643875416177\n",
      "Epoch: 983, Total Loss: 0.021030189236426987\n",
      "Epoch: 984, Total Loss: 0.01949000885882527\n",
      "Epoch: 985, Total Loss: 0.01670847757592859\n",
      "Epoch: 986, Total Loss: 0.014447405424070171\n",
      "Epoch: 987, Total Loss: 0.013107190398757164\n",
      "Epoch: 988, Total Loss: 0.011058620256432693\n",
      "Epoch: 989, Total Loss: 0.00948214276018022\n",
      "Epoch: 990, Total Loss: 0.008701467322184648\n",
      "Epoch: 991, Total Loss: 0.008664407178856907\n",
      "Epoch: 992, Total Loss: 0.007794976339565038\n",
      "Epoch: 993, Total Loss: 0.007419477821576554\n",
      "Epoch: 994, Total Loss: 0.006928764688581647\n",
      "Epoch: 995, Total Loss: 0.006188424906782234\n",
      "Epoch: 996, Total Loss: 0.005158523491944974\n",
      "Epoch: 997, Total Loss: 0.004097728389253493\n",
      "Epoch: 998, Total Loss: 0.003982343411929651\n",
      "Epoch: 999, Total Loss: 0.0035919747874514803\n",
      "Epoch: 1000, Total Loss: 0.00320548977853489\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_vector, model, total_loss_values, loss_values, material_loss_values, sobolev_loss_values, alpha_values_values = train_material(epochs=700, nodes=nodes, K=K, f=f, E=E, A=A, I=I, network_type='material')\n",
    "# input_vector, model = train(epochs=3000, nodes=nodes, K=K, f=f, E=E, A=A, I=I, use_residual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(nodes, material_params, model, uh_vem, K, f, concatanate=False):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure nodes and material_params are torch tensors\n",
    "    if isinstance(nodes, np.ndarray):\n",
    "        nodes = torch.tensor(nodes, dtype=torch.float32)\n",
    "    if isinstance(material_params, np.ndarray):\n",
    "        material_params = torch.tensor(material_params, dtype=torch.float32)\n",
    "\n",
    "    K = torch.tensor(K, dtype=torch.float32, requires_grad=True)\n",
    "    f = torch.tensor(f, dtype=torch.float32, requires_grad=True)\n",
    "    uh_vem = torch.tensor(uh_vem, dtype=torch.float32)\n",
    "\n",
    "    # Ensure gradients are not tracked during prediction\n",
    "    with torch.no_grad():\n",
    "        # Use the trained model to make predictions\n",
    "        if concatanate:\n",
    "            input_vector = torch.cat((nodes, material_params))\n",
    "            predicted_displacements = model(input_vector)\n",
    "        else:\n",
    "            predicted_displacements = model(nodes, material_params)\n",
    "\n",
    "    # Print or use the predicted displacements\n",
    "    print(\"Predicted displacements:\", predicted_displacements)\n",
    "\n",
    "    # Compute errors and ensure tensors are on the same device\n",
    "    l2_error = compute_l2_error(uh_vem, predicted_displacements).item()\n",
    "    energy_error = compute_energy_error(K, uh_vem, predicted_displacements).item()\n",
    "    h1_error = compute_h1_norm(K, uh_vem, predicted_displacements).item()\n",
    "\n",
    "    return predicted_displacements, l2_error, energy_error, h1_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted displacements: tensor([-2.5647e-03,  2.7030e-03, -3.0983e-03, -2.8989e-03, -8.7607e-04,\n",
      "         4.4130e-04,  1.0792e-03,  1.0221e-03, -9.3216e-04, -2.1064e-03,\n",
      "        -3.0641e-03,  4.0117e-04,  5.5611e-04,  8.7491e-04, -2.6712e-03,\n",
      "        -3.4678e-03, -7.3554e-03, -3.1225e-03, -3.6120e-04,  4.1276e-03,\n",
      "         5.1882e-03,  4.2145e-03, -5.3174e-03, -5.9819e-04,  7.9851e-03,\n",
      "         2.2203e-05,  7.9176e-03, -4.2540e-03, -5.4137e-03,  2.1839e-04,\n",
      "         1.4708e-03,  5.9421e-03, -1.9545e-03,  2.5734e-03, -3.5480e-03,\n",
      "         2.0289e-04,  3.1084e-04,  1.9980e-03, -2.2297e-03,  5.6203e-04,\n",
      "        -4.5100e-03,  2.3471e-03, -8.2970e-04, -1.4634e-03, -5.6397e-03,\n",
      "        -1.6515e-03, -1.3788e-03,  9.4146e-04, -6.3186e-03, -1.5363e-04,\n",
      "        -2.2167e-03,  2.4123e-03, -4.1433e-03,  1.3316e-03, -3.3496e-03,\n",
      "        -4.3657e-03,  1.7272e-03,  1.3216e-03,  4.8178e-04,  7.3184e-03,\n",
      "        -4.7446e-03,  1.0921e-03,  7.1210e-03,  3.0026e-03, -2.9507e-03,\n",
      "         3.8484e-04,  8.3627e-04,  6.9580e-03,  9.0784e-04,  1.2008e-03,\n",
      "         2.5833e-03, -4.1416e-03,  3.8228e-03, -6.6563e-03,  1.3695e-03])\n",
      "L2 error: 89.63257598876953\n",
      "Energy error: 4122828.5\n",
      "H1 error: 0.03026679717004299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r8/9jdwwqz11dq7_2m0n6cds_k40000gn/T/ipykernel_56698/787023990.py:33: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  U_fem = 0.5 * torch.matmul(uh_fem.T, torch.matmul(K, uh_fem))\n"
     ]
    }
   ],
   "source": [
    "material_params = torch.tensor([E , A , I ], dtype=torch.float32)\n",
    "nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "predicted_displacements, l2_error, energy_error, h1_error = test(nodes, material_params, model, uh_vem, K, f, concatanate=False)\n",
    "print(f\"L2 error: {l2_error}\")\n",
    "print(f\"Energy error: {energy_error}\")\n",
    "print(f\"H1 error: {h1_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################### Beam ##########################\n",
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.49889624e-08\n",
      " -1.47880917e-05  1.57050436e-07  7.76756694e-08 -2.95761834e-05\n",
      " -8.61228753e-07  1.81237667e-07 -4.07723211e-05  6.98715130e-08\n",
      " -1.93364355e-08 -4.47845507e-05  1.57186004e-06 -2.73125477e-10\n",
      " -4.52048263e-05 -1.68722778e-06 -6.44566470e-07 -4.56251019e-05\n",
      "  6.87871328e-06  2.76655931e-06 -4.60453775e-05 -3.41305808e-05\n",
      " -1.31217236e-05 -4.64656531e-05  1.61273983e-04 -1.30949837e-05\n",
      " -1.47506153e-05  9.30300354e-05 -1.30682438e-05 -2.72978130e-07\n",
      "  2.33747782e-05 -1.30415040e-05  2.21709451e-06 -2.87048097e-06\n",
      " -1.30147641e-05  1.13308844e-06 -5.21785148e-06 -1.29880242e-05\n",
      "  7.99785328e-08 -2.62331170e-06 -1.29612843e-05 -9.06325245e-08\n",
      "  1.84213935e-06 -1.29345444e-05 -1.03432502e-06 -8.80796316e-06\n",
      " -1.29078045e-05  3.36220484e-06  4.45639181e-05 -4.09141523e-06\n",
      "  2.94192923e-06  2.59300574e-05 -3.09238466e-08  2.52165363e-06\n",
      "  6.51673497e-06  6.81293692e-07  2.10137802e-06 -8.56133392e-07\n",
      "  3.82047929e-07  1.68110242e-06 -1.57497145e-06  9.98091213e-08\n",
      "  1.26082681e-06 -7.20077748e-07 -2.75174910e-09  8.40551209e-07\n",
      " -1.37547952e-07 -9.97262153e-09  4.20275605e-07  4.26422352e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      "#########################################################\n"
     ]
    }
   ],
   "source": [
    "num_elements_per_edge = 8\n",
    "\n",
    "# geometry data\n",
    "L = 2.0\n",
    "I = 36e-4\n",
    "A = 0.12\n",
    "\n",
    "# material data\n",
    "E = 29e6\n",
    "\n",
    "# Generate the geometry\n",
    "nodes, elements, supp = generate_geometry(num_elements_per_edge, L)\n",
    "\n",
    "# Plot the nodes\n",
    "plot_nodes(nodes, elements)\n",
    "\n",
    "\n",
    "# loads\n",
    "load = np.array([[2,3],[3,4]])\n",
    "q = -400\n",
    "t = 0\n",
    "f_dist = vem.buildBeamDistributedLoad(load,t,q,nodes)\n",
    "\n",
    "# stiffness matrix\n",
    "K = vem.buildGlobaBeamK(nodes, elements, E, A, I, 1)\n",
    "\n",
    "# apply DBC\n",
    "K, f = vem.applyDBCBeam(K, f_dist, supp)\n",
    "\n",
    "# solve\n",
    "print()\n",
    "print(\"######################### Beam ##########################\")\n",
    "uh_vem = np.linalg.solve(K,f)\n",
    "print(uh_vem)\n",
    "print(\"#########################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted displacements: tensor([-0.2641, -0.0069, -0.1010,  0.0774,  0.0473,  0.3178, -0.1058,  0.2614,\n",
      "         0.1467, -0.0363, -0.3185,  0.4608, -0.0464, -0.2549, -0.0570, -0.1230,\n",
      "        -0.1658,  0.0866,  0.1573, -0.0935, -0.0239,  0.0616,  0.0567,  0.1368,\n",
      "        -0.0385, -0.0688,  0.0121,  0.0165, -0.0276,  0.1441, -0.3054, -0.1822,\n",
      "        -0.0500,  0.1491,  0.1416,  0.0581,  0.0802, -0.2478,  0.0440,  0.1005,\n",
      "        -0.3354, -0.1285,  0.1234,  0.1105, -0.0361,  0.2591,  0.0673,  0.2585,\n",
      "        -0.0061, -0.3468, -0.0314,  0.2386,  0.0516,  0.1582, -0.2002,  0.0079,\n",
      "        -0.0920,  0.0618, -0.0212, -0.3106, -0.1208,  0.0571, -0.4285,  0.0405,\n",
      "         0.0618,  0.0712,  0.1378,  0.0176, -0.1828, -0.1211, -0.2613,  0.1353,\n",
      "        -0.0327,  0.1967, -0.1091])\n",
      "L2 error: 6277.92431640625\n",
      "Energy error: 16019860480.0\n",
      "H1 error: 1.4620044231414795\n"
     ]
    }
   ],
   "source": [
    "material_params = torch.tensor([E , A , I ], dtype=torch.float32)\n",
    "nodes = torch.tensor(nodes.flatten(), dtype=torch.float32)\n",
    "predicted_displacements, l2_error, energy_error, h1_error = test(nodes, material_params, model, uh_vem, K, f, concatanate=False)\n",
    "print(f\"L2 error: {l2_error}\")\n",
    "print(f\"Energy error: {energy_error}\")\n",
    "print(f\"H1 error: {h1_error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
